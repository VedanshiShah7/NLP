{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports go here\n",
    "import collections\n",
    "import random\n",
    "import sys\n",
    "\n",
    "def read_sentences(filepath):\n",
    "    \"\"\"\n",
    "    Reads contents of a file line by line.\n",
    "    Parameters:\n",
    "      filepath (str): file to read from\n",
    "    Return:\n",
    "      list of strings\n",
    "    \"\"\"\n",
    "    f = open(filepath, \"r\")\n",
    "    sentences = f.readlines()\n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_character(filepath):\n",
    "    \"\"\"\n",
    "    Reads contents of a script file line by line and sorts into \n",
    "    buckets based on speaker name.\n",
    "    Parameters:\n",
    "      filepath (str): file to read from\n",
    "    Return:\n",
    "      dict of strings to list of strings, the dialogue that speaker speaks\n",
    "    \"\"\"\n",
    "    char_data = {}\n",
    "    script_file = open(filepath, \"r\", encoding=\"utf-8\")\n",
    "    for line in script_file:\n",
    "        # extract the part between <speaker> tags\n",
    "        speakers = line[line.index(\n",
    "            \"<speakers>\") + len(\"<speakers>\"): line.index(\"</speakers>\")].strip()\n",
    "        if not speakers in char_data:\n",
    "            char_data[speakers] = []\n",
    "        char_data[speakers].append(line)\n",
    "    return char_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    # constants to define pseudo-word tokens\n",
    "    # access via self.UNK, for instance\n",
    "    UNK = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, n_gram, is_laplace_smoothing, line_begin=\"<line>\", line_end=\"</line>\"):\n",
    "        \"\"\"Initializes an untrained LanguageModel\n",
    "        Parameters:\n",
    "          n_gram (int): the n-gram order of the language model to create\n",
    "          is_laplace_smoothing (bool): whether or not to use Laplace smoothing\n",
    "          line_begin (str): the token designating the beginning of a line\n",
    "          line_end (str): the token designating the end of a line\n",
    "        \"\"\"\n",
    "        self.line_begin = line_begin\n",
    "        self.line_end = line_end\n",
    "        # your other code here\n",
    "        self.n_gram = n_gram\n",
    "        self.is_laplace_smoothing = is_laplace_smoothing\n",
    "        self.ngram_counts = {}\n",
    "        self.n_minus_one_counts = {}  # REMOVE: or None?\n",
    "        self.vocab = set()\n",
    "        self.model = None\n",
    "        self.tokenizer = lambda x: x.split()\n",
    "        self.total_count = {}\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "        has tokens that are white-space separated, has one sentence per line, and\n",
    "        that the sentences begin with line_begin and end with line_end\n",
    "        Parameters:\n",
    "          sentences (list): list of strings, one string per line in the training file\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        ngram_minus_one_list = []\n",
    "        ngram_list = []\n",
    "        all_tokens = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            all_tokens += tokens\n",
    "            count = collections.Counter(tokens)\n",
    "            count_dict = dict(count)\n",
    "\n",
    "            n_minus_1_gram = self.n_gram - 1\n",
    "            for i in range(len(tokens) - n_minus_1_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list_minus_one = tokens[i:i+n_minus_1_gram]\n",
    "                n_minus_one_gram_word = \" \".join(tokens_list_minus_one)\n",
    "                ngram_minus_one_list.append(n_minus_one_gram_word)\n",
    "\n",
    "            for i in range(len(tokens) - self.n_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list = tokens[i:i+self.n_gram]\n",
    "                ngram_word = \" \".join(tokens_list)\n",
    "                ngram_list.append(ngram_word)\n",
    "\n",
    "            n_minus_one_num = dict(collections.Counter(ngram_minus_one_list))\n",
    "            ngram_num = dict(collections.Counter(ngram_list))\n",
    "\n",
    "        # for k in count_dict:\n",
    "        #     if count_dict[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "        #         indx = tokens.index(k)\n",
    "        #         tokens[indx] = self.UNK\n",
    "\n",
    "        print('this is tokens')\n",
    "        print(tokens)\n",
    "\n",
    "        print('this is n-1')\n",
    "        print(n_minus_one_num)\n",
    "        # going through the dictionary and removing the tokens that only appear once\n",
    "        # get all keys that appear once\n",
    "        num_iter = 0\n",
    "        remove_list = []\n",
    "        for k,v in n_minus_one_num.items():\n",
    "            if v == 1 and k != self.line_begin and k != self.line_end:\n",
    "                remove_list.append(k)\n",
    "                # remove k from ngram_num\n",
    "                # n_minus_one_num.pop(k)\n",
    "                num_iter+=1\n",
    "                # add UNK to ngram_num\n",
    "        n_minus_one_num[self.UNK] = num_iter\n",
    "        for k in remove_list:\n",
    "            n_minus_one_num.pop(k)\n",
    "    \n",
    "        \n",
    "        # for k in n_minus_one_num:\n",
    "        #     print(k)\n",
    "        #     if n_minus_one_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "        #         # remove k from ngram_num\n",
    "        #         n_minus_one_num.pop(k)\n",
    "        #         # add UNK to ngram_num\n",
    "        #         n_minus_one_num[self.UNK] = 1\n",
    "\n",
    "        print('this is n')\n",
    "\n",
    "        num_iter_ngram = 0\n",
    "        remove_list_ngram = []\n",
    "        print(ngram_num)\n",
    "        for k,v in n_minus_one_num.items():\n",
    "            print(k)\n",
    "            if v == 1 and k != self.line_begin and k != self.line_end:\n",
    "                print(f'tokens{tokens}')\n",
    "                remove_list_ngram.append(k)\n",
    "                # indx = tokens.index(k)\n",
    "\n",
    "                # remove k from ngram_num\n",
    "                # ngram_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                num_iter_ngram+=1\n",
    "        ngram_num[self.UNK] = num_iter_ngram\n",
    "        for k in remove_list_ngram:\n",
    "            ngram_num.pop(k)\n",
    "\n",
    "        print('updated ngram')\n",
    "        print(ngram_num)\n",
    "        self.n_minus_one_counts = n_minus_one_num\n",
    "        self.ngram_counts = ngram_num\n",
    "        self.total_count = count_dict\n",
    "\n",
    "        # the vocab is the set of all tokens\n",
    "        print(f'--------- all tokens: {all_tokens}---------')\n",
    "        self.vocab = all_tokens\n",
    "\n",
    "        print(self.ngram_counts)\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
    "        Parameters:\n",
    "          sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
    "\n",
    "        Returns:\n",
    "          float: the probability value of the given string for this model\n",
    "        \"\"\"\n",
    "\n",
    "        score = 1\n",
    "        n_minus_one_count = self.n_minus_one_counts\n",
    "        ngram_count = self.ngram_counts\n",
    "        tokens = sentence.split()\n",
    "        total_words = len(self.vocab)\n",
    "        vocab_size = len(self.total_count)\n",
    "\n",
    "        if self.n_gram == 1:\n",
    "            for word in tokens:\n",
    "\n",
    "                if word not in ngram_count:\n",
    "                    word = self.UNK\n",
    "                    \n",
    "                if self.is_laplace_smoothing:\n",
    "                    prob = (ngram_count[word] + 1) / \\\n",
    "                        (total_words + vocab_size)\n",
    "                else:\n",
    "                    prob = ngram_count[word] / total_words\n",
    "                score = score * prob\n",
    "            return score\n",
    "\n",
    "        # if self.n_gram == 2:\n",
    "        else:\n",
    "            print('in 2')\n",
    "            for i in range(0, len(tokens)-1):\n",
    "                tokens_list = tokens[i: i+self.n_gram]\n",
    "                tokens_list_word = \" \".join(tokens_list)\n",
    "\n",
    "                for token in tokens_list:\n",
    "                    if token not in ngram_count:\n",
    "                        token = self.UNK\n",
    "                        \n",
    "                if tokens_list_word not in ngram_count:\n",
    "                    ngram_count[tokens_list_word] = self.UNK\n",
    "                    \n",
    "                if tokens_list_word not in n_minus_one_count:\n",
    "                    n_minus_one_count[tokens_list_word] = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    if (tokens_list_word) not in ngram_count:  # list of things\n",
    "                        print('here')\n",
    "                        \n",
    "                        print(f'token list {tokens_list_word}')\n",
    "                        print(f'ngrammm {ngram_count}')\n",
    "                        print(f'numerator: {ngram_count[tokens_list_word] +1}')\n",
    "                        print(f'denominator: {n_minus_one_count[tokens_list_word]+vocab_size}')\n",
    "                        # calculate the probability of the ngram\n",
    "                        prob = (ngram_count[tokens_list_word] + 1) / \\\n",
    "                            (n_minus_one_count[tokens_list_word] +\n",
    "                             vocab_size)\n",
    "                    else:\n",
    "                        print(f'numerator: {ngram_count[tokens_list_word]}')\n",
    "                        print(f'denominator: {n_minus_one_count[tokens_list_word]+vocab_size}')\n",
    "                        prob = (ngram_count[tokens_list_word] + 1) / \\\n",
    "                            (n_minus_one_count[tokens_list_word] +\n",
    "                             vocab_size)  # ngram + 1/ngram_mius_1+ 1+ vocab size\n",
    "                else:\n",
    "                    if (tokens_list_word) not in ngram_count:\n",
    "                        prob = 0\n",
    "                    else:\n",
    "                        prob = ngram_count[tokens_list_word] / \\\n",
    "                            n_minus_one_count[tokens_list_word]\n",
    "                score = score * prob\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    # constants to define pseudo-word tokens\n",
    "    # access via self.UNK, for instance\n",
    "    UNK = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, n_gram, is_laplace_smoothing, line_begin=\"<line>\", line_end=\"</line>\"):\n",
    "        \"\"\"Initializes an untrained LanguageModel\n",
    "        Parameters:\n",
    "          n_gram (int): the n-gram order of the language model to create\n",
    "          is_laplace_smoothing (bool): whether or not to use Laplace smoothing\n",
    "          line_begin (str): the token designating the beginning of a line\n",
    "          line_end (str): the token designating the end of a line\n",
    "        \"\"\"\n",
    "        self.line_begin = line_begin\n",
    "        self.line_end = line_end\n",
    "        # your other code here\n",
    "        self.n_gram = n_gram\n",
    "        self.is_laplace_smoothing = is_laplace_smoothing\n",
    "        self.ngram_counts = {}\n",
    "        self.n_minus_one_counts = {}  # REMOVE: or None?\n",
    "        self.vocab = set()\n",
    "        self.model = None\n",
    "        # tokenize without nltk\n",
    "        self.tokenizer = lambda x: x.split()\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "        has tokens that are white-space separated, has one sentence per line, and\n",
    "        that the sentences begin with line_begin and end with line_end\n",
    "        Parameters:\n",
    "          sentences (list): list of strings, one string per line in the training file\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        ngram_minus_one_list = []\n",
    "        ngram_list = []\n",
    "        all_tokens = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            all_tokens += tokens\n",
    "            count = collections.Counter(tokens)\n",
    "            count_dict = dict(count)\n",
    "\n",
    "            n_minus_1_gram = self.n_gram - 1\n",
    "            for i in range(len(tokens) - n_minus_1_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list_minus_one = tokens[i:i+n_minus_1_gram]\n",
    "                # print('ooooooooooooooo')\n",
    "                # print(f'tokens minus one {tokens_list_minus_one}')\n",
    "                n_minus_one_gram_word = \" \".join(tokens_list_minus_one)\n",
    "                ngram_minus_one_list.append(n_minus_one_gram_word)\n",
    "\n",
    "            for i in range(len(tokens) - self.n_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list = tokens[i:i+self.n_gram]\n",
    "                ngram_word = \" \".join(tokens_list)\n",
    "                ngram_list.append(ngram_word)\n",
    "\n",
    "            n_minus_one_num = dict(collections.Counter(ngram_minus_one_list))\n",
    "            ngram_num = dict(collections.Counter(ngram_list))\n",
    "\n",
    "        # for k in count_dict:\n",
    "        #     if count_dict[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "        #         indx = tokens.index(k)\n",
    "        #         tokens[indx] = self.UNK\n",
    "\n",
    "        # print('this is tokens')\n",
    "        # print(tokens)\n",
    "\n",
    "        print('this is n-1')\n",
    "        print(n_minus_one_num)\n",
    "        num_keys = 0\n",
    "        for k in list(n_minus_one_num.keys()):\n",
    "            if n_minus_one_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "                # remove k from ngram_num\n",
    "                n_minus_one_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                num_keys += 1\n",
    "        n_minus_one_num[self.UNK] = num_keys\n",
    "\n",
    "        print('this is n')\n",
    "\n",
    "        num_ngram_keys = 0\n",
    "        print(ngram_num)\n",
    "        for k in list(ngram_num.keys()):\n",
    "            if ngram_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "                print(f'tokens{tokens}')\n",
    "                # indx = tokens.index(k)\n",
    "\n",
    "                # remove k from ngram_num\n",
    "                ngram_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                num_ngram_keys += 1\n",
    "        ngram_num[self.UNK] = num_ngram_keys\n",
    "\n",
    "        print('updated ngram')\n",
    "        print(ngram_num)\n",
    "        self.n_minus_one_counts = n_minus_one_num\n",
    "        self.ngram_counts = ngram_num\n",
    "        self.total_count = count_dict\n",
    "\n",
    "        # the vocab is the set of all tokens\n",
    "        print(f'--------- all tokens: {all_tokens}---------')\n",
    "        self.vocab = all_tokens\n",
    "\n",
    "        print(self.ngram_counts)\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
    "        Parameters:\n",
    "          sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
    "\n",
    "        Returns:\n",
    "          float: the probability value of the given string for this model\n",
    "        \"\"\"\n",
    "\n",
    "        score = 1\n",
    "        n_minus_one_count = self.n_minus_one_counts\n",
    "        ngram_count = self.ngram_counts\n",
    "        tokens = sentence.split()\n",
    "        total_words = len(self.vocab)\n",
    "        vocab_size = len(self.total_count)\n",
    "\n",
    "        print(f' n minus one count {n_minus_one_count}')\n",
    "        print('+++++++++++++++++++++++++')\n",
    "        if self.n_gram == 1:\n",
    "            for word in tokens:\n",
    "\n",
    "                if word not in ngram_count:\n",
    "                    word = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    prob = (ngram_count[word] + 1) / \\\n",
    "                        (total_words + vocab_size)\n",
    "                else:\n",
    "                    prob = ngram_count[word] / total_words\n",
    "                score = score * prob\n",
    "            return score\n",
    "\n",
    "        # if self.n_gram == 2:\n",
    "        else:\n",
    "            print('in 2')\n",
    "            for i in range(0, len(tokens)-1):\n",
    "                tokens_list = tokens[i: i+self.n_gram]\n",
    "                tokens_list_word = \" \".join(tokens_list)\n",
    "                tokens_list_new = tokens[i: i+self.n_gram-1][0]\n",
    "                print(f'tokens lisT NEW: {tokens_list_new}')\n",
    "\n",
    "                for token in tokens_list:\n",
    "                    if token not in ngram_count:\n",
    "                        token = self.UNK\n",
    "                \n",
    "                if tokens_list_word not in ngram_count:\n",
    "                    tokens_list_word = self.UNK\n",
    "                    \n",
    "                if tokens_list_word not in n_minus_one_count:\n",
    "                    tokens_list_word = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    print('in laplace')\n",
    "                    if (tokens_list_word) not in ngram_count:  # list of things\n",
    "                        # calculate the probability of the ngram\n",
    "                        prob = (ngram_count[tokens_list_word] + 1) / \\\n",
    "                            (n_minus_one_count[tokens_list_word] +\n",
    "                             vocab_size)\n",
    "                    # if the ngram is not in the ngram count\n",
    "                    else:\n",
    "                        print('laplace else')\n",
    "                        print(f'numerator: {ngram_count[tokens_list_word]}')\n",
    "                        print(f'denomanator: {n_minus_one_count[tokens_list_word] + vocab_size}')\n",
    "                        print(ngram_count[tokens_list_word])\n",
    "                        print(vocab_size)\n",
    "                        prob = (ngram_count[tokens_list_word] + 1) / \\\n",
    "                            (n_minus_one_count[tokens_list_new] +\n",
    "                             vocab_size)  # ngram + 1/ngram_mius_1+ 1+ vocab size\n",
    "                        print(f' n minus one count {n_minus_one_count}')\n",
    "                        print(f' n count {ngram_count}')\n",
    "                        print('+++++++++++++++++++++++++')  \n",
    "                else:\n",
    "                    if (tokens_list_word) not in ngram_count:\n",
    "                        print('not laplace')\n",
    "                        prob = 0\n",
    "                    else:\n",
    "                        print('not laplace else')\n",
    "                        prob = ngram_count[tokens_list_word] / \\\n",
    "                            n_minus_one_count[tokens_list_word]\n",
    "                score = score * prob\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    # constants to define pseudo-word tokens\n",
    "    # access via self.UNK, for instance\n",
    "    UNK = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, n_gram, is_laplace_smoothing, line_begin=\"<line>\", line_end=\"</line>\"):\n",
    "        \"\"\"Initializes an untrained LanguageModel\n",
    "        Parameters:\n",
    "          n_gram (int): the n-gram order of the language model to create\n",
    "          is_laplace_smoothing (bool): whether or not to use Laplace smoothing\n",
    "          line_begin (str): the token designating the beginning of a line\n",
    "          line_end (str): the token designating the end of a line\n",
    "        \"\"\"\n",
    "        self.line_begin = line_begin\n",
    "        self.line_end = line_end\n",
    "        # your other code here\n",
    "        self.n_gram = n_gram\n",
    "        self.is_laplace_smoothing = is_laplace_smoothing\n",
    "        self.ngram_counts = {}\n",
    "        self.n_minus_one_counts = {}  # REMOVE: or None?\n",
    "        self.vocab = set()\n",
    "        self.model = None\n",
    "        # tokenize without nltk\n",
    "        self.tokenizer = lambda x: x.split()\n",
    "    def train(self, sentences):\n",
    "        \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "        has tokens that are white-space separated, has one sentence per line, and\n",
    "        that the sentences begin with line_begin and end with line_end\n",
    "        Parameters:\n",
    "            sentences (list): list of strings, one string per line in the training file\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        ngram_minus_one_list = []\n",
    "        ngram_list = []\n",
    "        all_tokens = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            all_tokens += tokens\n",
    "            count = collections.Counter(tokens)\n",
    "            count_dict = dict(count)\n",
    "\n",
    "            n_minus_1_gram = self.n_gram - 1\n",
    "            for i in range(len(tokens) - n_minus_1_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list_minus_one = tokens[i:i+n_minus_1_gram]\n",
    "                n_minus_one_gram_word = \" \".join(tokens_list_minus_one)\n",
    "                ngram_minus_one_list.append(n_minus_one_gram_word)\n",
    "\n",
    "            for i in range(len(tokens) - self.n_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list = tokens[i:i+self.n_gram]\n",
    "                ngram_word = \" \".join(tokens_list)\n",
    "                ngram_list.append(ngram_word)\n",
    "\n",
    "            n_minus_one_num = dict(collections.Counter(ngram_minus_one_list))\n",
    "            ngram_num = dict(collections.Counter(ngram_list))\n",
    "\n",
    "        # for k in count_dict:\n",
    "        #     if count_dict[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "        #         indx = tokens.index(k)\n",
    "        #         tokens[indx] = self.UNK\n",
    "\n",
    "        print('this is tokens')\n",
    "        print(tokens)\n",
    "\n",
    "        print('this is n-1')\n",
    "        print(n_minus_one_num)\n",
    "        for k in list(n_minus_one_num.keys()):\n",
    "            if n_minus_one_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "                # remove k from ngram_num\n",
    "                n_minus_one_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                n_minus_one_num[self.UNK] = 1\n",
    "\n",
    "        print('this is n')\n",
    "\n",
    "        print(ngram_num)\n",
    "        for k in list(ngram_num.keys()):\n",
    "            if ngram_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "                print(f'tokens{tokens}')\n",
    "                # indx = tokens.index(k)\n",
    "\n",
    "                # remove k from ngram_num\n",
    "                ngram_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                ngram_num[self.UNK] = 1\n",
    "\n",
    "        print('updated ngram')\n",
    "        print(ngram_num)\n",
    "        self.n_minus_one_counts = n_minus_one_num\n",
    "        self.ngram_counts = ngram_num\n",
    "        self.total_count = count_dict\n",
    "\n",
    "        # the vocab is the set of all tokens\n",
    "        print(f'--------- all tokens: {all_tokens}---------')\n",
    "        self.vocab = all_tokens\n",
    "\n",
    "        print(self.ngram_counts)\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
    "        Parameters:\n",
    "            sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
    "\n",
    "        Returns:\n",
    "            float: the probability value of the given string for this model\n",
    "        \"\"\"\n",
    "\n",
    "        score = 1\n",
    "        n_minus_one_count = self.n_minus_one_counts\n",
    "        ngram_count = self.ngram_counts\n",
    "        tokens = sentence.split()\n",
    "        total_words = len(self.vocab)\n",
    "        vocab_size = len(self.total_count)\n",
    "\n",
    "        if self.n_gram == 1:\n",
    "            for word in tokens:\n",
    "\n",
    "                if word not in ngram_count:\n",
    "                    word = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    prob = (ngram_count[word] + 1) / \\\n",
    "                        (total_words + vocab_size)\n",
    "                else:\n",
    "                    prob = ngram_count[word] / total_words\n",
    "                score = score * prob\n",
    "            return score\n",
    "\n",
    "        # if self.n_gram == 2:\n",
    "        else:\n",
    "            print('in 2')\n",
    "            for i in range(0, len(tokens)-1):\n",
    "                tokens_list = tokens[i: i+self.n_gram]\n",
    "                tokens_list_word = \" \".join(tokens_list)\n",
    "\n",
    "                # for token in tokens_list:\n",
    "                #     if token not in ngram_count:\n",
    "                #         token = self.UNK\n",
    "\n",
    "                if tokens_list_word not in ngram_count:\n",
    "                    tokens_list_word = self.UNK\n",
    "\n",
    "                # if tokens_list_word not in n_minus_one_count:\n",
    "                #     tokens_list_word = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    if (tokens_list_word) not in ngram_count:  # list of things\n",
    "                        # calculate the probability of the ngram\n",
    "                        prob = (ngram_count[tokens_list_word] + 1) / \\\n",
    "                            (n_minus_one_count[tokens_list_word] +\n",
    "                                vocab_size)\n",
    "                    else:\n",
    "                        prob = (ngram_count[tokens_list_word] + 1) / \\\n",
    "                            (n_minus_one_count[tokens_list_word] +\n",
    "                                vocab_size)  # ngram + 1/ngram_mius_1+ 1+ vocab size\n",
    "                else:\n",
    "                    if (tokens_list_word) not in ngram_count:\n",
    "                        prob = 0\n",
    "                    else:\n",
    "                        prob = ngram_count[tokens_list_word] / \\\n",
    "                            n_minus_one_count[tokens_list_word]\n",
    "                score = score * prob\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is n-1\n",
      "{'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, 'today': 1}\n",
      "this is n\n",
      "{'<s> i': 1, 'i am': 2, 'am sam': 1, 'sam </s>': 1, '<s> sam': 1, 'sam i': 1, 'am today': 1, 'today </s>': 1}\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "updated ngram\n",
      "{'i am': 2, '<UNK>': 7}\n",
      "--------- all tokens: ['<s>', 'i', 'am', 'sam', '</s>', '<s>', 'sam', 'i', 'am', 'today', '</s>']---------\n",
      "{'i am': 2, '<UNK>': 7}\n",
      " n minus one count {'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, '<UNK>': 1}\n",
      "+++++++++++++++++++++++++\n",
      "in 2\n",
      "tokens lisT NEW: <s>\n",
      "in laplace\n",
      "laplace else\n",
      "numerator: 7\n",
      "denomanator: 7\n",
      "7\n",
      "6\n",
      " n minus one count {'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, '<UNK>': 1}\n",
      " n count {'i am': 2, '<UNK>': 7}\n",
      "+++++++++++++++++++++++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(2, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns_mixed.txt\")\n",
    "lm.train(sentences)\n",
    "# (0 + 1) / (2 + 6)\n",
    "lm.score(\"<s> flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is n-1\n",
      "{'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, 'today': 1}\n",
      "this is n\n",
      "{'<s> i': 1, 'i am': 2, 'am sam': 1, 'sam </s>': 1, '<s> sam': 1, 'sam i': 1, 'am today': 1, 'today </s>': 1}\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "tokens['<s>', 'sam', 'i', 'am', 'today', '</s>']\n",
      "updated ngram\n",
      "{'i am': 2, '<UNK>': 7}\n",
      "--------- all tokens: ['<s>', 'i', 'am', 'sam', '</s>', '<s>', 'sam', 'i', 'am', 'today', '</s>']---------\n",
      "{'i am': 2, '<UNK>': 7}\n",
      " n minus one count {'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, '<UNK>': 1}\n",
      "+++++++++++++++++++++++++\n",
      "in 2\n",
      "tokens lisT NEW: <s>\n",
      "in laplace\n",
      "laplace else\n",
      "numerator: 7\n",
      "denomanator: 7\n",
      "7\n",
      "6\n",
      " n minus one count {'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, '<UNK>': 1}\n",
      " n count {'i am': 2, '<UNK>': 7}\n",
      "+++++++++++++++++++++++++\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(2, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns_mixed.txt\")\n",
    "lm.train(sentences)\n",
    "# (0 + 1) / (2 + 6)\n",
    "lm.score(\"<s> flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, sentences):\n",
    "        \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "        has tokens that are white-space separated, has one sentence per line, and\n",
    "        that the sentences begin with line_begin and end with line_end\n",
    "        Parameters:\n",
    "          sentences (list): list of strings, one string per line in the training file\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            ngram_list = []\n",
    "            for i in range(0, len(tokens) - self.n_gram):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list = tokens[i:i+self.n_gram]\n",
    "                ngram_word = sentence = ' '.join(tokens_list)\n",
    "                ngram_list.append(ngram_word)\n",
    "                \n",
    "\n",
    "            unigram_num = collections.Counter(dict(tokens))\n",
    "            ngram_num = collections.Counter(dict(ngram_list))\n",
    "\n",
    "            self.unigram_counts = unigram_num\n",
    "            self.ngram_counts = ngram_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(self, sentence):\n",
    "        \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
    "        Parameters:\n",
    "          sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
    "\n",
    "        Returns:\n",
    "          float: the probability value of the given string for this model\n",
    "        \"\"\"\n",
    "\n",
    "        probability_so_far = 1\n",
    "        word_probabilities = self.unigram_counts\n",
    "        bigram_probabilities = self.ngram_counts\n",
    "        tokens = sentence.split()\n",
    "        types = len(word_probabilities)\n",
    "\n",
    "        for i in range(0, len(tokens)-1):\n",
    "            tokens_list = tokens[i+self.n_gram, i]\n",
    "            current_word = tokens[i+self.n_gram]\n",
    "            prev_word = tokens[i]\n",
    "\n",
    "            if current_word not in word_probabilities:\n",
    "                current_word = self.UNK\n",
    "            if prev_word not in word_probabilities:\n",
    "                prev_word = self.UNK\n",
    "            if self.is_laplace_smoothing:\n",
    "                if (tokens_list) not in bigram_probabilities:\n",
    "                    prob = 1/(word_probabilities[prev_word] + types)\n",
    "                else:\n",
    "                    prob = (bigram_probabilities[tokens_list] + 1) / (\n",
    "                        word_probabilities[prev_word] + types)\n",
    "            else:\n",
    "                if (tokens_list) not in bigram_probabilities:\n",
    "                    prob = 0\n",
    "                else:\n",
    "                    prob = bigram_probabilities[tokens_list] / \\\n",
    "                        word_probabilities[prev_word]\n",
    "            probability_so_far = probability_so_far * prob\n",
    "        return probability_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(\"training_files/iamsam.txt\")\n",
    "LanguageModel(3, True, line_begin=\"<s>\", line_end=\"</s>\").train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'sam', 'i', 'am', '</s>']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am', '</s>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'sam', 'i'], ['sam', 'i', 'am'], ['i', 'am', '</s>'], ['am', '</s>']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 1, 'sam': 1, 'i': 1, 'am': 1, '</s>': 1}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    ngram_list = []\n",
    "    for i in range(0, len(tokens) - 1):\n",
    "        # get the tokens from current to next\n",
    "        tokens_list = tokens[i:i+3]\n",
    "        ngram_list.append(tokens_list)\n",
    "\n",
    "    unigram_num = dict(collections.Counter(tokens))\n",
    "unigram_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LanguageModel:\n",
    "    # constants to define pseudo-word tokens\n",
    "    # access via self.UNK, for instance\n",
    "    UNK = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, n_gram, is_laplace_smoothing, line_begin=\"<line>\", line_end=\"</line>\"):\n",
    "        \"\"\"Initializes an untrained LanguageModel\n",
    "        Parameters:\n",
    "          n_gram (int): the n-gram order of the language model to create\n",
    "          is_laplace_smoothing (bool): whether or not to use Laplace smoothing\n",
    "          line_begin (str): the token designating the beginning of a line\n",
    "          line_end (str): the token designating the end of a line\n",
    "        \"\"\"\n",
    "        self.line_begin = line_begin\n",
    "        self.line_end = line_end\n",
    "        # your other code here\n",
    "        self.n_gram = n_gram\n",
    "        self.is_laplace_smoothing = is_laplace_smoothing\n",
    "        self.ngram_counts = {}\n",
    "        self.n_minus_one_counts = {}  # REMOVE: or None?\n",
    "        self.vocab = set()\n",
    "        self.model = None\n",
    "        # tokenize without nltk\n",
    "        self.tokenizer = lambda x: x.split()\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "        has tokens that are white-space separated, has one sentence per line, and\n",
    "        that the sentences begin with line_begin and end with line_end\n",
    "        Parameters:\n",
    "          sentences (list): list of strings, one string per line in the training file\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        ngram_minus_one_list = []\n",
    "        ngram_list = []\n",
    "        all_tokens = []\n",
    "        \n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            all_tokens += tokens\n",
    "            \n",
    "\n",
    "            n_minus_1_gram = self.n_gram - 1\n",
    "            for i in range(len(tokens) - n_minus_1_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list_minus_one = tokens[i:i+n_minus_1_gram]\n",
    "                n_minus_one_gram_word = \" \".join(tokens_list_minus_one)\n",
    "                ngram_minus_one_list.append(n_minus_one_gram_word)\n",
    "\n",
    "            for i in range(len(tokens) - self.n_gram + 1):\n",
    "                # get the tokens from current to next\n",
    "                tokens_list = tokens[i:i+self.n_gram]\n",
    "                ngram_word = \" \".join(tokens_list)\n",
    "                ngram_list.append(ngram_word)\n",
    "\n",
    "            n_minus_one_num = dict(collections.Counter(ngram_minus_one_list))\n",
    "            # print(f'n-1 num: `{n_minus_one_num}`')\n",
    "            ngram_num = dict(collections.Counter(ngram_list))\n",
    "            # print(f'n num: `{ngram_num}`')\n",
    "\n",
    "        n_1_num_unk = 0\n",
    "        \n",
    "        for k in list(n_minus_one_num.keys()):\n",
    "            if n_minus_one_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "                # remove k from ngram_num\n",
    "                n_minus_one_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                n_1_num_unk += 1\n",
    "            n_minus_one_num[self.UNK] = n_1_num_unk\n",
    "\n",
    "        num_unk = 0\n",
    "        for k in list(ngram_num.keys()):\n",
    "            if ngram_num[k] == 1 and k != self.line_begin and k != self.line_end:\n",
    "                # remove k from ngram_num\n",
    "                ngram_num.pop(k)\n",
    "                # add UNK to ngram_num\n",
    "                num_unk += 1\n",
    "            ngram_num[self.UNK] = num_unk\n",
    "        \n",
    "        if ngram_num[self.UNK] == 0:\n",
    "            ngram_num.pop(self.UNK)\n",
    "            \n",
    "        if n_minus_one_num[self.UNK] == 0:\n",
    "            n_minus_one_num.pop(self.UNK)\n",
    "\n",
    "        count = collections.Counter(tokens)\n",
    "        count_dict = dict(count)\n",
    "        self.n_minus_one_counts = n_minus_one_num\n",
    "        self.ngram_counts = ngram_num\n",
    "        self.total_count = count_dict\n",
    "\n",
    "        # the vocab is the set of all tokens\n",
    "        self.vocab = all_tokens\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"Calculates the probability score for a given string representing a single sentence.\n",
    "        Parameters:\n",
    "          sentence (str): a sentence with tokens separated by whitespace to calculate the score of\n",
    "\n",
    "        Returns:\n",
    "          float: the probability value of the given string for this model\n",
    "        \"\"\"\n",
    "\n",
    "        score = 1\n",
    "        n_minus_one_count = self.n_minus_one_counts\n",
    "        ngram_count = self.ngram_counts\n",
    "        tokens = sentence.split()\n",
    "        total_words = sum(self.ngram_counts.values())\n",
    "        vocab_size = len(self.n_minus_one_counts)\n",
    "        # print(f'total count is : {self.total_count}')\n",
    "        print(f'ngram count is : {self.ngram_counts}')\n",
    "        \n",
    "        if self.n_gram == 1:\n",
    "            # vocab_size = len(self.vocab)\n",
    "            vocab_size = len(self.ngram_counts)\n",
    "            \n",
    "            for word in tokens:\n",
    "\n",
    "                if word not in ngram_count:\n",
    "                    word = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    # print(f'numerator: {ngram_count[word] + 1}')\n",
    "                    # print(f'denominator: {total_words + vocab_size}')\n",
    "                    # print(f'total word: {total_words}')\n",
    "                    # print(f'vocab size: {vocab_size}')\n",
    "                    \n",
    "                    prob = (ngram_count[word] + 1) / \\\n",
    "                        (total_words + vocab_size)\n",
    "                else:\n",
    "                    # print(f'numerator: {ngram_count[word]}')\n",
    "                    # print(f'deno: {total_words}')\n",
    "                    prob = ngram_count[word] / total_words\n",
    "                score = score * prob\n",
    "            return score\n",
    "\n",
    "        else:\n",
    "            print('in 2')\n",
    "            for i in range(0, len(tokens)-1):\n",
    "                tokens_list = tokens[i: i+self.n_gram]\n",
    "                tokens_list_new = tokens[i: i+self.n_gram-1][0]\n",
    "                tokens_list_word = \" \".join(tokens_list)\n",
    "\n",
    "                # if tokens_list_word not in ngram_count:\n",
    "                #     tokens_list_word = self.UNK\n",
    "\n",
    "                if self.is_laplace_smoothing:\n",
    "                    if (tokens_list_word) not in ngram_count:\n",
    "                        # calculate the probability of the ngram\n",
    "                        # print('in laplace if')\n",
    "                        # print(f'numerator: {ngram_count.get(tokens_list_word, 0) + 1}')\n",
    "                        # print(f'denominator: {n_minus_one_count.get(tokens_list_word, 0) +vocab_size}')\n",
    "                        # print(f'nminusonecount tokenlistword: {n_minus_one_count.get(tokens_list_word, 0)}')\n",
    "                        # print(f'vocab size: {vocab_size}')\n",
    "                        prob = (ngram_count.get(tokens_list_word, 0) + 1) / \\\n",
    "                            (n_minus_one_count.get(tokens_list_new, 0) +\n",
    "                             vocab_size)\n",
    "                        print(f'the probability is: {prob}')\n",
    "                    else:\n",
    "                        print('in laplace else')\n",
    "                        # print(f'numerator: {ngram_count[tokens_list_word] + 1}')\n",
    "                        # print(f'denominator: {n_minus_one_count[tokens_list_new] + vocab_size}')\n",
    "                        # print(f'{n_minus_one_count[tokens_list_new] } n-1 token list')\n",
    "                        # print(f'vocab size: {vocab_size}')\n",
    "                        # print(f'ngram count: {ngram_count}')\n",
    "                        # print(f'ngram_minus_1 count: {n_minus_one_count}')\n",
    "                        # this works if I remove the +1 -- but i need to keep it because of laplace smoothing\n",
    "                        prob = (ngram_count.get(tokens_list_word, 0) + 1) / \\\n",
    "                            (n_minus_one_count.get(tokens_list_new, 0) +\n",
    "                             vocab_size)\n",
    "                    print(prob)\n",
    "                    # score = score * prob\n",
    "                else:\n",
    "                    if (tokens_list_word) not in ngram_count:\n",
    "                        prob = 0\n",
    "                    else:\n",
    "                        print(f'{ n_minus_one_count.get(tokens_list_new)} : ')\n",
    "                        \n",
    "                        print('in else')\n",
    "                        prob = ngram_count[tokens_list_word] / \\\n",
    "                            n_minus_one_count[tokens_list_new]\n",
    "                score *= prob       \n",
    "                \n",
    "                # print(score)\n",
    "            return score\n",
    "        \n",
    "    def generate_sentence(self):\n",
    "        \"\"\"Generates a single sentence from a trained language model using the Shannon technique.\n",
    "\n",
    "        Returns:\n",
    "          str: the generated sentence\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # check if begin is in ngram_dict\n",
    "        begin = self.line_begin\n",
    "        if begin in ngram_dict:\n",
    "        \n",
    "            if self.n_gram == 1:\n",
    "                \n",
    "                # getting the probability list\n",
    "                unigram_dict = self.ngram_counts\n",
    "                # getting the begining value\n",
    "                begin = self.line_begin\n",
    "                # setting the current token to begin\n",
    "                current_token = begin\n",
    "                sentence = begin\n",
    "                \n",
    "                # storing the value to add back later\n",
    "                beg_val = unigram_dict[begin]\n",
    "                # remove line_begin from unigram_dict\n",
    "                unigram_dict.pop(self.line_begin)\n",
    "                \n",
    "                # create a probability list\n",
    "                prob_list = []\n",
    "\n",
    "                # calculate the probability of each unigram\n",
    "                for unigram in unigram_dict:\n",
    "                    prob = self.score(unigram)\n",
    "                    # storing it in the list\n",
    "                    prob_list.append(prob)\n",
    "                \n",
    "                # converting the dictionary to a list to use in the random function\n",
    "                unigram_list = list(unigram_dict.keys())\n",
    "                \n",
    "                # sampling the unigram list based on the probability list without the begining token\n",
    "                while current_token != self.line_end:\n",
    "                    # using np.random.choice as it is much faster than random.choices\n",
    "                    current_token = np.random.choice(unigram_list, p=prob_list)\n",
    "                    # adding the randomly picked current token to the sentence\n",
    "                    sentence = sentence + \" \" + current_token\n",
    "                \n",
    "                # adding back the value of line_begin to the unigram_dict \n",
    "                # so it is not mutated for future calls            \n",
    "                unigram_dict[self.line_begin] = beg_val\n",
    "                return sentence\n",
    "\n",
    "            else:\n",
    "                # getting the probability list\n",
    "                # n_minus_1_gram_dict = self.n_minus_one_counts\n",
    "                ngram_dict = self.ngram_counts\n",
    "\n",
    "                # getting the begining value and setting it as the starting value\n",
    "                begin = self.line_begin\n",
    "                \n",
    "                sentence = begin\n",
    "\n",
    "                current_token = self.line_begin\n",
    "            \n",
    "                beg_val = ngram_dict[begin]\n",
    "                # remove line_begin from unigram_dict\n",
    "                ngram_dict.pop(self.line_begin)\n",
    "                \n",
    "                # creating a probability list\n",
    "                prob_list = []\n",
    "                \n",
    "                # calculating the probability of each n minus 1 gram\n",
    "                for n_gram in ngram_dict:\n",
    "                    prob = self.score(n_gram)\n",
    "                    prob_list.append(prob)\n",
    "                    \n",
    "                # converting the dictionary to a list to use in the random function\n",
    "                n_gram_list = list(ngram_dict.keys())\n",
    "                \n",
    "                # creating a counter for line end tokens\n",
    "                line_end_tks = 0 \n",
    "                \n",
    "                while line_end_tks != (self.n_gram - 1):\n",
    "                    current_token = np.random.choice(n_gram_list, p=prob_list)\n",
    "                    if current_token == self.line_end:\n",
    "                        line_end_tks += 1\n",
    "                    sentence = sentence + \" \" + current_token\n",
    "                    \n",
    "                \n",
    "                # adding back the value of line_begin to the unigram_dict \n",
    "                # so it is not mutated for future calls   \n",
    "                ngram_dict[self.line_begin] = beg_val\n",
    "                \n",
    "                # tacking on the begin tokens to the beginning of the sentence\n",
    "                for i in range(0, self.n_gram - 1):\n",
    "                    line_begins = self.line_begin\n",
    "                    sentence = line_begins + \" \" + sentence\n",
    "                \n",
    "                return sentence\n",
    "        else:\n",
    "            if self.n_gram == 1:\n",
    "                sentence = '<s> </s>'\n",
    "            else:\n",
    "                line_begins = self.line_begin\n",
    "                line_end = self.line_end\n",
    "                sentence = line_begins * (self.n_gram - 1) + \" \" + line_end * (self.n_gram - 1)\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"Generates n sentences from a trained language model using the Shannon technique.\n",
    "        Parameters:\n",
    "            n (int): the number of sentences to generate\n",
    "\n",
    "        Returns:\n",
    "            list: a list containing strings, one per generated sentence\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "\n",
    "        for i in range(0, n):\n",
    "            new_sentence = self.generate_sentence()\n",
    "            print(f'thus the new sentence is: {new_sentence}')\n",
    "            sentences.append(new_sentence)\n",
    "        print(f'thus the senteces are: {sentences}')\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ngram_dict' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vedanshi/Desktop/NLP/HW 2/lm_friends.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lm\u001b[39m.\u001b[39mtrain(sentences)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# sentences should only contain unk tokens\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sents \u001b[39m=\u001b[39m lm\u001b[39m.\u001b[39;49mgenerate(\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/Users/vedanshi/Desktop/NLP/HW 2/lm_friends.ipynb Cell 16\u001b[0m in \u001b[0;36mLanguageModel.generate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=306'>307</a>\u001b[0m sentences \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=308'>309</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=309'>310</a>\u001b[0m     new_sentence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_sentence()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=310'>311</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthus the new sentence is: \u001b[39m\u001b[39m{\u001b[39;00mnew_sentence\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=311'>312</a>\u001b[0m     sentences\u001b[39m.\u001b[39mappend(new_sentence)\n",
      "\u001b[1;32m/Users/vedanshi/Desktop/NLP/HW 2/lm_friends.ipynb Cell 16\u001b[0m in \u001b[0;36mLanguageModel.generate_sentence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m \u001b[39m# check if begin is in ngram_dict\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m begin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_begin\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39mif\u001b[39;00m begin \u001b[39min\u001b[39;00m ngram_dict:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_gram \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m         \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m         \u001b[39m# getting the probability list\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X43sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m         unigram_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_counts\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ngram_dict' referenced before assignment"
     ]
    }
   ],
   "source": [
    "lm = LanguageModel(1, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns.txt\")\n",
    "lm.train(sentences)\n",
    "\n",
    "# sentences should only contain unk tokens\n",
    "sents = lm.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and then for bigrams\n",
    "lm = LanguageModel(2, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns.txt\")\n",
    "lm.train(sentences)\n",
    "score('UNK')\n",
    "\n",
    "# sentences should only contain unk tokens\n",
    "sents = lm.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count is : {'<s>': 1, 'goose': 1, 'or': 1, 'moose': 1, '</s>': 1}\n",
      "in 2\n",
      "in laplace if\n",
      "numerator: 1\n",
      "denominator: 3\n",
      "nminusonecount tokenlistword: 0\n",
      "vocab size: 3\n",
      "the probability is: 0.3333333333333333\n",
      "0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(1, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns.txt\")\n",
    "lm.train(sentences)\n",
    "\n",
    "# sentences should only contain unk tokens\n",
    "# sents = lm.generate(5)\n",
    "# print(sents)\n",
    "# for sent in sents:\n",
    "#     words = sent.split()\n",
    "#     if len(words) > 2:\n",
    "#         for word in words[1:-1]:\n",
    "#             word.upper()\n",
    "\n",
    "# # probability of unk should be v high\n",
    "# score = lm.score(\"porcupine\")\n",
    "# # (6 + 1) / (10 + 3)\n",
    "# score\n",
    "\n",
    "# # and then for bigrams\n",
    "# lm = LanguageModel(2, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "# sentences = read_sentences(\"training_files/unknowns.txt\")\n",
    "# lm.train(sentences)\n",
    "\n",
    "# # sentences should only contain unk tokens\n",
    "# sents = lm.generate(5)\n",
    "# for sent in sents:\n",
    "#     words = sent.split()\n",
    "#     if len(words) > 2:\n",
    "#         for word in words[1:-1]:\n",
    "#             word.upper()\n",
    "\n",
    "# # probability of unk should be v high\n",
    "# score = lm.score(\"porcupine wombat\")\n",
    "# # (4 + 1) / (6 + 3)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " python test_minitrainingprovided.py \n",
    "$ python lm_friends.py 1 training_files/friends_train.txt training_files/friends_test.txt line \n",
    "$ python lm_friends.py 2 training_files/friends_train.txt training_files/friends_test.txt line \n",
    "$ python lm_friends.py 1 training_files/friends_train.txt training_files/friends_test.txt line character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ((2 + 1) / (4 + 6)) * ((4 + 1) / (4 + 6)) * ((2 + 1) / (4 + 6))\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2, '<UNK>': 1}"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-1 num: `{'': 6}`\n",
      "n num: `{'<s>': 1, 'i': 1, 'am': 1, 'sam': 1, '</s>': 1}`\n",
      "n-1 num: `{'': 12}`\n",
      "n num: `{'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2}`\n",
      "n-1 num: `{'': 18}`\n",
      "n num: `{'<s>': 3, 'i': 3, 'am': 3, 'sam': 2, '</s>': 3, 'ham': 1}`\n",
      "n-1 num: `{'': 24}`\n",
      "n num: `{'<s>': 4, 'i': 4, 'am': 4, 'sam': 2, '</s>': 4, 'ham': 2}`\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "numerator: 3\n",
      "denominator: 25\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "numerator: 5\n",
      "denominator: 25\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "numerator: 3\n",
      "denominator: 25\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "numerator: 5\n",
      "denominator: 25\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "numerator: 5\n",
      "denominator: 25\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "numerator: 5\n",
      "denominator: 25\n",
      "['ham', 'i', 'sam', '<s>', '</s>', 'am']\n",
      "[0.12, 0.2, 0.12, 0.2, 0.2, 0.2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vedanshi/Desktop/NLP/HW 2/lm_friends.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sentences \u001b[39m=\u001b[39m read_sentences(\u001b[39m\"\u001b[39m\u001b[39mtraining_files/iamsam2.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lm\u001b[39m.\u001b[39mtrain(sentences)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sents \u001b[39m=\u001b[39m lm\u001b[39m.\u001b[39;49mgenerate(\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mlen\u001b[39m(sents)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(sents)\n",
      "\u001b[1;32m/Users/vedanshi/Desktop/NLP/HW 2/lm_friends.ipynb Cell 17\u001b[0m in \u001b[0;36mLanguageModel.generate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=277'>278</a>\u001b[0m sentences \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=279'>280</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=280'>281</a>\u001b[0m     new_sentence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_sentence()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=281'>282</a>\u001b[0m     sentences\u001b[39m.\u001b[39mappend(new_sentence)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sentences\n",
      "\u001b[1;32m/Users/vedanshi/Desktop/NLP/HW 2/lm_friends.ipynb Cell 17\u001b[0m in \u001b[0;36mLanguageModel.generate_sentence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39mprint\u001b[39m(prob_list)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39mwhile\u001b[39;00m tok \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m</s>\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m     w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(unigram_list, p\u001b[39m=\u001b[39;49mprob_list)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m     prob_so_far \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vedanshi/Desktop/NLP/HW%202/lm_friends.ipynb#X32sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32mmtrand.pyx:939\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "lm = LanguageModel(1, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/iamsam2.txt\")\n",
    "lm.train(sentences)\n",
    "sents = lm.generate(2)\n",
    "len(sents)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-1 num: `{'<s>': 1, 'i': 1, 'am': 1, 'sam': 1, '</s>': 1}`\n",
      "n num: `{'<s> i': 1, 'i am': 1, 'am sam': 1, 'sam </s>': 1}`\n",
      "n-1 num: `{'<s>': 2, 'i': 2, 'am': 2, 'sam': 2, '</s>': 2}`\n",
      "n num: `{'<s> i': 1, 'i am': 2, 'am sam': 1, 'sam </s>': 1, '<s> sam': 1, 'sam i': 1, 'am </s>': 1}`\n",
      "n-1 num: `{'<s>': 3, 'i': 3, 'am': 3, 'sam': 2, '</s>': 3, 'ham': 1}`\n",
      "n num: `{'<s> i': 2, 'i am': 3, 'am sam': 1, 'sam </s>': 1, '<s> sam': 1, 'sam i': 1, 'am </s>': 1, 'am ham': 1, 'ham </s>': 1}`\n",
      "n-1 num: `{'<s>': 4, 'i': 4, 'am': 4, 'sam': 2, '</s>': 4, 'ham': 2}`\n",
      "n num: `{'<s> i': 2, 'i am': 4, 'am sam': 1, 'sam </s>': 1, '<s> sam': 1, 'sam i': 1, 'am </s>': 2, 'am ham': 1, 'ham </s>': 1, '<s> ham': 1, 'ham i': 1}`\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "in 2\n",
      "in laplace else\n",
      "numerator: 3\n",
      "denominator: 10\n",
      "4 n-1 token list\n",
      "vocab size: 6\n",
      "ngram count: {'<s> i': 2, 'i am': 4, 'am </s>': 2, '<UNK>': 8}\n",
      "ngram_minus_1 count: {'<s>': 4, 'i': 4, 'am': 4, 'sam': 2, '</s>': 4, 'ham': 2}\n",
      "total count is : {'<s>': 1, 'ham': 1, 'i': 1, 'am': 1, '</s>': 1}\n",
      "in 2\n",
      "in laplace else\n",
      "numerator: 3\n",
      "denominator: 10\n",
      "4 n-1 token list\n",
      "vocab size: 6\n",
      "ngram count: {'<s> i': 2, 'i am': 4, 'am </s>': 2, '<UNK>': 8}\n",
      "ngram_minus_1 count: {'<s>': 4, 'i': 4, 'am': 4, 'sam': 2, '</s>': 4, 'ham': 2}\n",
      "in laplace else\n",
      "numerator: 5\n",
      "denominator: 10\n",
      "4 n-1 token list\n",
      "vocab size: 6\n",
      "ngram count: {'<s> i': 2, 'i am': 4, 'am </s>': 2, '<UNK>': 8}\n",
      "ngram_minus_1 count: {'<s>': 4, 'i': 4, 'am': 4, 'sam': 2, '</s>': 4, 'ham': 2}\n",
      "in laplace else\n",
      "numerator: 3\n",
      "denominator: 10\n",
      "4 n-1 token list\n",
      "vocab size: 6\n",
      "ngram count: {'<s> i': 2, 'i am': 4, 'am </s>': 2, '<UNK>': 8}\n",
      "ngram_minus_1 count: {'<s>': 4, 'i': 4, 'am': 4, 'sam': 2, '</s>': 4, 'ham': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004629629629629629"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(2, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/iamsam2.txt\")\n",
    "lm.train(sentences)\n",
    "# (2 + 1) / (4 + 6)\n",
    "lm.score(\"<s> i\")\n",
    "# ((2 + 1) / (4 + 6)) * ((4 + 1) / (4 + 6)) * ((2 + 1) / (4 + 6))\n",
    "lm.score(\"<s> i am </s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 2\n",
      "in else\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(2, False, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns_mixed.txt\")\n",
    "lm.train(sentences)\n",
    "# ((0) / (2))\n",
    "lm.score(\"<s> flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 2\n",
      "in laplace if\n",
      "numerator: 1\n",
      "denominator: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(2, True, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/unknowns_mixed.txt\")\n",
    "lm.train(sentences)\n",
    "# (0 + 1) / (2 + 6)\n",
    "lm.score(\"<s> flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 2\n",
      "4 : \n",
      "in else\n",
      "in 2\n",
      "4 : \n",
      "in else\n",
      "4 : \n",
      "in else\n",
      "4 : \n",
      "in else\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LanguageModel(2, False, line_begin=\"<s>\", line_end=\"</s>\")\n",
    "sentences = read_sentences(\"training_files/iamsam2.txt\")\n",
    "lm.train(sentences)\n",
    "# (2) / (4)\n",
    "lm.score(\"<s> i\")\n",
    "# (2 / 4) * (4 / 4) * (2 / 4)\n",
    "lm.score(\"<s> i am </s>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
