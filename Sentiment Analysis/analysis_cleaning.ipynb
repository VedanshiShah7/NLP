{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - bag-of-words baseline\n",
      "Base precision: 0.553763440860215\n",
      "Base recall: 0.9809523809523809\n",
      "Base f1: 0.7079037800687284\n",
      "Logistic Regression Classifier\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.14328484 -0.18776618 -0.04112838 -0.01828445 -0.00435504 -0.01503128\n",
      "  0.00458965]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.14338049 -0.18691384 -0.05432421 -0.01815834 -0.00442221 -0.01648799\n",
      "  0.01139621]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.14346046 -0.18648903 -0.06036364 -0.01808562 -0.00445556 -0.01796295\n",
      "  0.01812064]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 0.14389553 -0.18571806 -0.06540108 -0.01785944 -0.00451305 -0.02802813\n",
      "  0.06324355]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.16448167 -0.20494944 -0.09350451 -0.05372674  0.00181123 -0.06814724\n",
      "  0.13602598]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7692307692307693\n",
      "FINAL RECALL: 0.6666666666666666\n",
      "FINAL F1: 0.7142857142857142\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.16559345 -0.20380024 -0.09356542 -0.05293274  0.00148625 -0.09450464\n",
      "  0.25355748]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7692307692307693\n",
      "FINAL RECALL: 0.6666666666666666\n",
      "FINAL F1: 0.7142857142857142\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.16650518 -0.20286141 -0.09360852 -0.05227531  0.00122029 -0.11614031\n",
      "  0.35003396]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7692307692307693\n",
      "FINAL RECALL: 0.6666666666666666\n",
      "FINAL F1: 0.7142857142857142\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 1.69627686e-01 -1.99671331e-01 -9.37089968e-02 -4.99791030e-02\n",
      "  3.13676136e-04 -1.90390721e-01  6.81134243e-01]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7752808988764045\n",
      "FINAL RECALL: 0.6571428571428571\n",
      "FINAL F1: 0.711340206185567\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.19651127 -0.32188235 -0.13850388 -0.08232534  0.00957138 -0.11957766\n",
      "  0.37510534]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7534246575342466\n",
      "FINAL RECALL: 0.5238095238095238\n",
      "FINAL F1: 0.6179775280898877\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.19866472 -0.3192586  -0.13921127 -0.07972959  0.00877347 -0.17481904\n",
      "  0.61870309]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7567567567567568\n",
      "FINAL RECALL: 0.5333333333333333\n",
      "FINAL F1: 0.6256983240223464\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.19998637 -0.3175446  -0.13957896 -0.07803649  0.00825616 -0.21017063\n",
      "  0.77465385]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7534246575342466\n",
      "FINAL RECALL: 0.5238095238095238\n",
      "FINAL F1: 0.6179775280898877\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 0.20211948 -0.31454998 -0.1400375  -0.07509822  0.00736431 -0.27041403\n",
      "  1.04062737]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7567567567567568\n",
      "FINAL RECALL: 0.5333333333333333\n",
      "FINAL F1: 0.6256983240223464\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.15615283 -0.18339821 -0.0742798  -0.03468326 -0.00465065 -0.03838595\n",
      "  0.05939424]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.15667213 -0.18284614 -0.074279   -0.0343746  -0.00474178 -0.05134873\n",
      "  0.1172606 ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.15714125 -0.18235097 -0.07425094 -0.03409567 -0.00482371 -0.06305772\n",
      "  0.1695311 ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 0.15937584 -0.18001938 -0.07410345 -0.03276121 -0.00521157 -0.11875934\n",
      "  0.418206  ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7352941176470589\n",
      "FINAL RECALL: 0.7142857142857143\n",
      "FINAL F1: 0.7246376811594202\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: rename this file to textclassify_model.py\n",
    "\n",
    "# feel free to include more imports as needed here\n",
    "# these are the ones that we used for the base model\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Your name and file comment here:\n",
    "Vedanshi Shah & Byron Pham\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Cite your sources here:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Implement your functions that are not methods of the TextClassify class here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_tuples_from_file(training_file_path):\n",
    "    \"\"\"\n",
    "    Generates tuples from file formated like:\n",
    "    id\\ttext\\tlabel\n",
    "    Parameters:\n",
    "      training_file_path - str path to file to read in\n",
    "    Return:\n",
    "      a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "    \"\"\"\n",
    "    f = open(training_file_path, \"r\", encoding=\"utf8\")\n",
    "    listOfExamples = []\n",
    "    for review in f:\n",
    "        if len(review.strip()) == 0:\n",
    "            continue\n",
    "        dataInReview = review.split(\"\\t\")\n",
    "        for i in range(len(dataInReview)):\n",
    "            # remove any extraneous whitespace\n",
    "            dataInReview[i] = dataInReview[i].strip()\n",
    "        t = tuple(dataInReview)\n",
    "        listOfExamples.append(t)\n",
    "    f.close()\n",
    "    return listOfExamples\n",
    "\n",
    "\n",
    "def precision(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the precision for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double precision (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and predicted_labels[i] == '1':\n",
    "            true_pos += 1\n",
    "        elif gold_labels[i] == '0' and predicted_labels[i] == '1':\n",
    "            false_pos += 1\n",
    "    if true_pos == 0 and false_pos == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the recall for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double recall (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and predicted_labels[i] == '1':\n",
    "            true_pos += 1\n",
    "        elif gold_labels[i] == '1' and predicted_labels[i] == '0':\n",
    "            false_neg += 1\n",
    "    if true_pos == 0 and false_neg == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the f1 for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double f1 (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    precision_val = precision(gold_labels, predicted_labels)\n",
    "    recall_val = recall(gold_labels, predicted_labels)\n",
    "    if precision_val + recall_val == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = (2 * precision_val * recall_val) / (precision_val + recall_val)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def precision_multiclass(gold_labels, classified_labels):\n",
    "    \"\"\" Gold labels is a list of strings of the true labels\n",
    "        Classified labels is a list of strings of the labels assigned by the classifier\n",
    "        Returns the precision as a float\n",
    "\n",
    "    Args:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        classified_labels (list): a corresponding list of labels predicted by the system\n",
    "\n",
    "    Returns:\n",
    "        double: double precision from 0 to 1\n",
    "    \"\"\"\n",
    "    # Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    num_correct = 0\n",
    "    num_predicted = 0\n",
    "\n",
    "    # loop through each claass\n",
    "    for label in set(classified_labels):\n",
    "        # calculate the number of times this class was predicted\n",
    "        predicted_count = classified_labels.count(label)\n",
    "        # calculcate the number of times this class was predicted correctly\n",
    "        correct_count = sum([1 for i in range(\n",
    "            len(gold_labels)) if gold_labels[i] == label and classified_labels[i] == label])\n",
    "\n",
    "        # add to the overall counters\n",
    "        num_correct += correct_count\n",
    "        num_predicted += predicted_count\n",
    "\n",
    "    # calculate the precision\n",
    "    if num_predicted == 0:\n",
    "        multi_precision = 0\n",
    "    else:\n",
    "        multi_precision = num_correct / num_predicted\n",
    "\n",
    "    return multi_precision\n",
    "\n",
    "\n",
    "def recall_multi(gold_labels, classified_labels):\n",
    "    \"\"\"gold labels is a list of strings of the true labels\n",
    "        classified labels is a list of strings of the labels assigned by the classifier\n",
    "        returns the recall as a float\n",
    "\n",
    "    Args:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        classified_labels (list): a corresponding list of labels predicted by the system\n",
    "\n",
    "    Returns:\n",
    "        float: recall as a float\n",
    "    \"\"\"\n",
    "    multiclass_recall = 0\n",
    "    class_labels = set(gold_labels)\n",
    "    num_gold = {label: 0 for label in class_labels}\n",
    "    num_correct = {label: 0 for label in class_labels}\n",
    "\n",
    "    for gold_label, classified_label in zip(gold_labels, classified_labels):\n",
    "        num_gold[gold_label] += 1\n",
    "        if gold_label == classified_label:\n",
    "            num_correct[gold_label] += 1\n",
    "\n",
    "    for label in class_labels:\n",
    "        if num_gold[label] > 0:\n",
    "            recall += num_correct[label] / num_gold[label]\n",
    "    multiclass_recall /= len(class_labels)\n",
    "\n",
    "    return multiclass_recall\n",
    "\n",
    "\n",
    "def f1_multi(gold_labels, classified_labels):\n",
    "    \"\"\"gold labels is a list of strings of the true labels\n",
    "        classified labels is a list of strings of the labels assigned by the classifier\n",
    "        returns the f1 as a float\n",
    "\n",
    "    Args:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        classified_labels (list): a corresponding list of labels predicted by the system\n",
    "\n",
    "    Returns:\n",
    "        float: f1 as a float\n",
    "    \"\"\"\n",
    "    precision = precision_multiclass(gold_labels, classified_labels)\n",
    "    recall = recall_multi(gold_labels, classified_labels)\n",
    "    multi_f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    return multi_f1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement any other non-required functions here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "implement your TextClassify class here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextClassify:\n",
    "\n",
    "    def __init__(self):\n",
    "        # do whatever you need to do to set up your class here\n",
    "        self.words_0 = Counter()\n",
    "        self.words_1 = Counter()\n",
    "\n",
    "        self.prior_0 = 0\n",
    "        self.prior_1 = 0\n",
    "\n",
    "        self.word_data = {0: self.words_0, 1: self.words_1}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        # TODO: Ask if this should be a self. or not\n",
    "\n",
    "        # calculate prior for each class\n",
    "        count_0 = 0\n",
    "        count_1 = 0\n",
    "        for example in examples:\n",
    "            if example[2] == \"0\":\n",
    "                count_0 += 1\n",
    "            else:\n",
    "                count_1 += 1\n",
    "\n",
    "        self.prior_0 = count_0 / len(examples)\n",
    "        self.prior_1 = count_1 / len(examples)\n",
    "\n",
    "        # update bag of words counts in self.word_data\n",
    "        # word_data format:\n",
    "        # { 0: Counter(), 1: Counter() }\n",
    "\n",
    "        for example in examples:\n",
    "            words = example[1].split()\n",
    "            self.vocab.update(words)\n",
    "            if example[2] == '0':\n",
    "                self.words_0.update(Counter(words))\n",
    "            else:\n",
    "                self.words_1.update(Counter(words))\n",
    "\n",
    "        self.word_data = {'0': self.words_0, '1': self.words_1}\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        \"\"\"\n",
    "        word_probs = {'0': 1, '1': 1}\n",
    "\n",
    "        word_list = data.split()\n",
    "\n",
    "        for word in word_list:\n",
    "            if word in self.words_0:\n",
    "                word_probs['0'] *= (self.words_0[word] + 1) / \\\n",
    "                    (sum(self.words_0.values()) + len(self.vocab))\n",
    "            elif word in self.vocab:\n",
    "                word_probs['0'] *= 1 / \\\n",
    "                    (sum(self.words_0.values()) + len(self.vocab))\n",
    "\n",
    "            if word in self.words_1:\n",
    "                word_probs['1'] *= (self.words_1[word] + 1) / \\\n",
    "                    (sum(self.words_1.values()) + len(self.vocab))\n",
    "            elif word in self.vocab:\n",
    "                word_probs['1'] *= 1 / \\\n",
    "                    (sum(self.words_1.values()) + len(self.vocab))\n",
    "\n",
    "        # multiply these by the prior\n",
    "        word_probs['0'] *= self.prior_0\n",
    "        word_probs['1'] *= self.prior_1\n",
    "\n",
    "        return word_probs\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "\n",
    "        score = self.score(data)\n",
    "        return '0' if score['0'] > score['1'] else '1'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of your TextClassifyImproved model more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "\n",
    "        data_list = data.split()\n",
    "        return [(d, True) for d in data_list]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Naive Bayes - bag-of-words baseline\"\n",
    "\n",
    "\n",
    "class TextClassifyImproved:\n",
    "    # count(positive words), count(negative words), 'no' in str,\n",
    "    # count of 1st and 2nd pronouns, if ! doc, log of length\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lexicon = self.read_lexicon('vader_lexicon.txt')\n",
    "        self.weights = []\n",
    "        self.theta = {}\n",
    "\n",
    "    def read_lexicon(self, filepath) -> dict:\n",
    "        output = {}\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split('\\t')\n",
    "                output[l[0]] = l[1]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def k_fold(all_examples, k):\n",
    "        \"\"\"\"\n",
    "        all examples is a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        containing all examples from the train and dev sets\n",
    "\n",
    "        @return a list of lists containing k sublists where each sublist is one \"fold\" in the given data\n",
    "        \"\"\"\n",
    "\n",
    "        # shuffle the examples randomly\n",
    "        random.shuffle(all_examples)\n",
    "\n",
    "        # split the examples into k folds\n",
    "        fold_size = len(all_examples) // k\n",
    "        folds = []\n",
    "        for i in range(k):\n",
    "            folds.append(all_examples[i * fold_size:(i + 1) * fold_size])\n",
    "        return folds\n",
    "\n",
    "        # # divide the shuffled examples into k folds\n",
    "        # folds = [all_examples[i::k] for i in range(k)]\n",
    "        # return folds\n",
    "\n",
    "    def train(self, examples, epoch, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        docs = [example[1] for example in examples]\n",
    "        labels = [example[2] for example in examples]\n",
    "\n",
    "        # featurize the training data\n",
    "        docs_featurized = [[f[1] for f in self.featurize(doc)] for doc in docs]\n",
    "\n",
    "        # create a vocabulary from the training data\n",
    "        # set of all unique words in the training data\n",
    "        vocab = set()\n",
    "        for doc in docs:\n",
    "            vocab.update(doc.split())\n",
    "\n",
    "        y = [1 if label == '1' else 0 for label in labels]\n",
    "\n",
    "        # set a flag to use logistic or apply kfold - true or false\n",
    "        # implements\n",
    "        # dev_data = self.k_fold(examples, 5)\n",
    "        # # evaluate the model on the validation data\n",
    "        # y_true = [example[2] for example in dev_data]\n",
    "        # y_pred = [self.train_logistic_regression(\n",
    "        #     example[1]) for example in dev_data]\n",
    "        # self.true = y_true\n",
    "        # self.pred = y_pred\n",
    "\n",
    "        # train the model with logistic regression\n",
    "        learning_rates = [0.01, 0.1, 1]\n",
    "        epochs = [10, 50, 100]\n",
    "\n",
    "        theta_dict = {}\n",
    "        \n",
    "        # for lr in learning_rates:\n",
    "            # for ep in epochs:\n",
    "        theta = self.train_logistic_regression(docs_featurized, y, learning_rate, epoch)\n",
    "                # theta_dict[lr, ep] = theta\n",
    "                \n",
    "        self.weights = theta\n",
    "    \n",
    "        # print(theta_dict)\n",
    "\n",
    "    def train_logistic_regression(self, x, y, learning_rate=0.001, num_epoch=100):\n",
    "        # initialise\n",
    "        n_features = len(x[0])\n",
    "        theta = np.zeros(n_features)\n",
    "\n",
    "        # perform gradient descent\n",
    "        for epoch in range(num_epoch):\n",
    "            for i in range(len(x)):\n",
    "                x_i = x[i]  # features\n",
    "                y_i = y[i]  # label\n",
    "                h_i = 1/(1 + np.exp(-np.dot(theta, x_i)))  # score\n",
    "                gradient = np.multiply((h_i - y_i), x_i)\n",
    "                theta = theta - learning_rate * gradient\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        you will compute e ^ (log(p(c)) + sum(log(p(w_i | c))) here\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        return a dictionary of the values of P(data | c)  for each class,\n",
    "        as in section 4.3 of the textbook e.g. {\"0\": 0.000061, \"1\": 0.000032}\n",
    "        \"\"\"\n",
    "        features = self.featurize(data)\n",
    "        assert (len(self.weights) == len(features))\n",
    "        feature_vals = [f[1] for f in features]\n",
    "\n",
    "        dot_prod = np.dot(self.weights, feature_vals)\n",
    "        score = 1 / (1 + math.exp(-1 * dot_prod))\n",
    "        return score\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "        prob = self.score(data)\n",
    "        return '0' if prob <= 0.5 else '1'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of this class more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "        # features = [pos_words, neg_words, num_nos,\n",
    "        #             num_1_and_2, num_exclam, log_length]\n",
    "\n",
    "        features = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        features[5] = math.log(len(data.split()))\n",
    "        features[4] = data.count('!')\n",
    "\n",
    "        for word in data.split():\n",
    "            if word in self.lexicon:\n",
    "                if float(self.lexicon[word]) > 0:\n",
    "                    features[0] += 1\n",
    "                elif float(self.lexicon[word]) < 0:\n",
    "                    features[1] += 1\n",
    "            if word.lower() == 'no':\n",
    "                features[2] += 1\n",
    "            if word.lower() in [\n",
    "                'me', 'i', 'my', 'myself', 'mine',\n",
    "                'we', 'us', 'our', 'ourselves', 'ours',\n",
    "                'you', 'your', 'yourself', 'yourselves'\n",
    "            ]:\n",
    "                features[3] += 1\n",
    "\n",
    "        return [\n",
    "            ('pos_words', features[0]),\n",
    "            ('neg_words', features[1]),\n",
    "            ('num_nos', features[2]),\n",
    "            ('num_1_2_pronouns', features[3]),\n",
    "            ('num_exclm', features[4]),\n",
    "            ('log_length', features[5]),\n",
    "            ('bias', 1)\n",
    "        ]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression Classifier\"\n",
    "\n",
    "    def describe_experiments(self):\n",
    "        s = \"\"\"\n",
    "    Description of your experiments and their outcomes here.\n",
    "    \"\"\"\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "training = 'training_files/movie_reviews_train.txt'\n",
    "testing = 'training_files/movie_reviews_dev.txt'\n",
    "\n",
    "classifier = TextClassify()\n",
    "print(classifier)\n",
    "# do the things that you need to with your base class\n",
    "examples_train_base = generate_tuples_from_file(training)\n",
    "classifier.train(examples_train_base)\n",
    "examples_dev_base = generate_tuples_from_file(testing)\n",
    "y_labels = [e[2] for e in examples_dev_base]\n",
    "y_pred = []\n",
    "for example in examples_dev_base:\n",
    "    y_pred.append(classifier.classify(example[1]))\n",
    "\n",
    "# report precision, recall, f1\n",
    "base_precision = precision(y_labels, y_pred)\n",
    "base_recall = recall(y_labels, y_pred)\n",
    "base_f1 = f1(y_labels, y_pred)\n",
    "\n",
    "print(f'Base precision: {base_precision}')\n",
    "print(f'Base recall: {base_recall}')\n",
    "print(f'Base f1: {base_f1}')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Improved model\n",
    "improved = TextClassifyImproved()\n",
    "print(improved)\n",
    "\n",
    "# do the things that you need to with your improved class\n",
    "examples_train = generate_tuples_from_file(training)\n",
    "learning_rate = [0.0001, 0.002, 0.005, 0.001]\n",
    "epochs = [50, 100, 150, 500]\n",
    "import pandas as pd\n",
    "lr_list = []\n",
    "epoch_list = []\n",
    "prec_lst = []\n",
    "rec_lst = []\n",
    "f1_lst = []\n",
    "df = pd.DataFrame(columns=['learning_rate', 'epochs', 'precision', 'recall', 'f1'])\n",
    "i = 0\n",
    "for lr in learning_rate:\n",
    "    for ep in epochs:\n",
    "        print(f'LEARNING RATE: {lr}')\n",
    "        print(f'EPOCHS: {ep}')\n",
    "        lr_list.append(lr)\n",
    "        epoch_list.append(ep)\n",
    "        improved.train(examples_train, ep, lr)\n",
    "        print(f'LEARNED WEIGHTS: {improved.weights}')\n",
    "\n",
    "        examples_dev = generate_tuples_from_file(testing)\n",
    "        y_labels = [e[2] for e in examples_dev]\n",
    "        y_pred = []\n",
    "        for example in examples_dev:\n",
    "            y_pred.append(improved.classify(example[1]))\n",
    "\n",
    "        # report a summary of your experiments/features here\n",
    "        print(improved.describe_experiments())\n",
    "\n",
    "        # report final precision, recall, f1 (for your best model)\n",
    "        # precision = tp / (tp+fp)\n",
    "        final_precision = precision(y_labels, y_pred)\n",
    "        prec_lst.append(final_precision)\n",
    "        # recall = tp / (tp+fn)\n",
    "        final_recall = recall(y_labels, y_pred)\n",
    "        rec_lst.append(final_recall)\n",
    "        # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        final_f1 = f1(y_labels, y_pred)\n",
    "        f1_lst.append(final_f1)\n",
    "\n",
    "        print(f'FINAL PRECISION: {final_precision}')\n",
    "        print(f'FINAL RECALL: {final_recall}')\n",
    "        print(f'FINAL F1: {final_f1}')\n",
    "        df.loc[i] = [lr, ep, final_precision, final_recall, final_f1]\n",
    "        i+=1\n",
    "        print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.711340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.625698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.625698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.724638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  epochs  precision    recall        f1\n",
       "0          0.0001    50.0   0.702703  0.742857  0.722222\n",
       "1          0.0001   100.0   0.702703  0.742857  0.722222\n",
       "2          0.0001   150.0   0.702703  0.742857  0.722222\n",
       "3          0.0001   500.0   0.702703  0.742857  0.722222\n",
       "4          0.0020    50.0   0.769231  0.666667  0.714286\n",
       "5          0.0020   100.0   0.769231  0.666667  0.714286\n",
       "6          0.0020   150.0   0.769231  0.666667  0.714286\n",
       "7          0.0020   500.0   0.775281  0.657143  0.711340\n",
       "8          0.0050    50.0   0.753425  0.523810  0.617978\n",
       "9          0.0050   100.0   0.756757  0.533333  0.625698\n",
       "10         0.0050   150.0   0.753425  0.523810  0.617978\n",
       "11         0.0050   500.0   0.756757  0.533333  0.625698\n",
       "12         0.0010    50.0   0.745098  0.723810  0.734300\n",
       "13         0.0010   100.0   0.745098  0.723810  0.734300\n",
       "14         0.0010   150.0   0.745098  0.723810  0.734300\n",
       "15         0.0010   500.0   0.735294  0.714286  0.724638"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['f1'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# include epochs in the plot\n",
    "df.sort_values(by=['learning_rate'], inplace=True)\n",
    "plt.plot(df['learning_rate'], df['precision'], label='precision', marker='o', color='red')\n",
    "plt.plot(df['learning_rate'], df['recall'], label='recall', marker='o', color='blue')\n",
    "plt.plot(df['learning_rate'], df['f1'], label='f1', marker='o', color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Rate vs Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the yelp data\n",
    "import pandas as pd\n",
    "\n",
    "df_yelp = pd.read_csv('training_files/yelp.csv')\n",
    "df_yelp.head()\n",
    "df_yelp = df_yelp[['user_id', 'text', 'stars']]\n",
    "df_yelp.to_csv('training_files/yelp.txt', index=False)\n",
    "\n",
    "# make sure the text column is clean and in one line\n",
    "df_yelp['text'] = df_yelp['text'].apply(lambda x: x.replace('\\r', ' ').replace('\\n', ' '))\n",
    "\n",
    "# get the first 800 reviews as training data\n",
    "df_yelp_train = df_yelp[:800]\n",
    "df_yelp_train.to_csv('training_files/yelp_train.txt', index=False, sep='\\t')\n",
    "\n",
    "# get the last 200 reviews as testing data\n",
    "df_yelp_test = df_yelp[800:]\n",
    "df_yelp_test.to_csv('training_files/yelp_test.txt', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caa917194a3f9b49aa7c5f08ecf9e4b4c66d3db34522bc007e94fdaaccb6f325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
