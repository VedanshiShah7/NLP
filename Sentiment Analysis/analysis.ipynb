{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: rename this file to textclassify_model.py\n",
    "\n",
    "# feel free to include more imports as needed here\n",
    "# these are the ones that we used for the base model\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "from torch import sigmoid\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Your name and file comment here:\n",
    "Vedanshi Shah & Byron Pham\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Cite your sources here:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Implement your functions that are not methods of the TextClassify class here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_tuples_from_file(training_file_path):\n",
    "    \"\"\"\n",
    "    Generates tuples from file formated like:\n",
    "    id\\ttext\\tlabel\n",
    "    Parameters:\n",
    "      training_file_path - str path to file to read in\n",
    "    Return:\n",
    "      a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "    \"\"\"\n",
    "    f = open(training_file_path, \"r\", encoding=\"utf8\")\n",
    "    listOfExamples = []\n",
    "    for review in f:\n",
    "        if len(review.strip()) == 0:\n",
    "            continue\n",
    "        dataInReview = review.split(\"\\t\")\n",
    "        for i in range(len(dataInReview)):\n",
    "            # remove any extraneous whitespace\n",
    "            dataInReview[i] = dataInReview[i].strip()\n",
    "        t = tuple(dataInReview)\n",
    "        listOfExamples.append(t)\n",
    "    f.close()\n",
    "    return listOfExamples\n",
    "\n",
    "\n",
    "def precision(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the precision for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double precision (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and predicted_labels[i] == '1':\n",
    "            true_pos += 1\n",
    "        elif gold_labels[i] == '0' and predicted_labels[i] == '1':\n",
    "            false_pos += 1\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the recall for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double recall (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and predicted_labels[i] == '1':\n",
    "            true_pos += 1\n",
    "        elif gold_labels[i] == '1' and predicted_labels[i] == '0':\n",
    "            false_neg += 1\n",
    "\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the f1 for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double f1 (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    precision_val = precision(gold_labels, predicted_labels)\n",
    "    recall_val = recall(gold_labels, predicted_labels)\n",
    "    if precision_val + recall_val == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = (2 * precision_val * recall_val) / (precision_val + recall_val)\n",
    "    return f1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement any other non-required functions here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "implement your TextClassify class here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextClassify:\n",
    "\n",
    "    def __init__(self):\n",
    "        # do whatever you need to do to set up your class here\n",
    "        self.words_0 = Counter()\n",
    "        self.words_1 = Counter()\n",
    "\n",
    "        self.prior_0 = 0\n",
    "        self.prior_1 = 0\n",
    "\n",
    "        self.word_data = {0: self.words_0, 1: self.words_1}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        # TODO: Ask if this should be a self. or not\n",
    "\n",
    "        # calculate prior for each class\n",
    "        count_0 = 0\n",
    "        count_1 = 0\n",
    "        for example in examples:\n",
    "            if example[2] == \"0\":\n",
    "                count_0 += 1\n",
    "            else:\n",
    "                count_1 += 1\n",
    "\n",
    "        self.prior_0 = count_0 / len(examples)\n",
    "        self.prior_1 = count_1 / len(examples)\n",
    "\n",
    "        # update bag of words counts in self.word_data\n",
    "        # word_data format:\n",
    "        # { 0: Counter(), 1: Counter() }\n",
    "\n",
    "        for example in examples:\n",
    "            words = example[1].split()\n",
    "            self.vocab.update(words)\n",
    "            if example[2] == '0':\n",
    "                self.words_0.update(Counter(words))\n",
    "            else:\n",
    "                self.words_1.update(Counter(words))\n",
    "\n",
    "        self.word_data = {'0': self.words_0, '1': self.words_1}\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        \"\"\"\n",
    "        word_probs = {'0': 1, '1': 1}\n",
    "\n",
    "        word_list = data.split()\n",
    "\n",
    "        for word in word_list:\n",
    "            if word in self.words_0:\n",
    "                word_probs['0'] *= (self.words_0[word] + 1) / \\\n",
    "                    (sum(self.words_0.values()) + len(self.vocab))\n",
    "            elif word in self.vocab:\n",
    "                word_probs['0'] *= 1 / \\\n",
    "                    (sum(self.words_0.values()) + len(self.vocab))\n",
    "\n",
    "            if word in self.words_1:\n",
    "                word_probs['1'] *= (self.words_1[word] + 1) / \\\n",
    "                    (sum(self.words_1.values()) + len(self.vocab))\n",
    "            elif word in self.vocab:\n",
    "                word_probs['1'] *= 1 / \\\n",
    "                    (sum(self.words_1.values()) + len(self.vocab))\n",
    "\n",
    "        # multiply these by the prior\n",
    "        word_probs['0'] *= self.prior_0\n",
    "        word_probs['1'] *= self.prior_1\n",
    "\n",
    "        return word_probs\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "\n",
    "        score = self.score(data)\n",
    "        return '0' if score['0'] > score['1'] else '1'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of your TextClassifyImproved model more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "\n",
    "        data_list = data.split()\n",
    "        return [(d, True) for d in data_list]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Naive Bayes - bag-of-words baseline\"\n",
    "\n",
    "\n",
    "class TextClassifyImproved:\n",
    "    # count(positive words), count(negative words), 'no' in str,\n",
    "    # count of 1st and 2nd pronouns, if ! doc, log of length\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lexicon = self.read_lexicon('vader_lexicon.txt')\n",
    "        self.weights = []\n",
    "\n",
    "    def read_lexicon(self, filepath) -> dict:\n",
    "        output = {}\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split('\\t')\n",
    "                output[l[0]] = l[1]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        docs = [example[1] for example in examples]\n",
    "        labels = [example[2] for example in examples]\n",
    "\n",
    "        # featurize the training data\n",
    "        docs_featurized = [[f[1] for f in self.featurize(doc)] for doc in docs]\n",
    "\n",
    "        # create a vocabulary from the training data\n",
    "        # set of all unique words in the training data\n",
    "        vocab = set()\n",
    "        for doc in docs:\n",
    "            vocab.update(doc.split())\n",
    "\n",
    "        # x = []\n",
    "        # for doc in docs_featurized:\n",
    "        #     x = [0] * len(vocab)\n",
    "        #     for word in vocab:\n",
    "        #         if word in doc:\n",
    "        #             x[vocab.index(word)] = 1\n",
    "        #     x.append(1)\n",
    "\n",
    "        y = [1 if label == '1' else 0 for label in labels]\n",
    "\n",
    "        # train the model with logistic regression\n",
    "        theta = self.train_logistic_regression(docs_featurized, y)\n",
    "        self.weights = theta\n",
    "\n",
    "    def train_logistic_regression(self, x, y, learning_rate=0.001, num_epoch=100):\n",
    "        # initialise\n",
    "        n_features = len(x[0])\n",
    "        theta = np.zeros(n_features)\n",
    "\n",
    "        # perform gradient descent\n",
    "        for epoch in range(num_epoch):\n",
    "            for i in range(len(x)):\n",
    "                x_i = x[i]  # features\n",
    "                y_i = y[i]  # label\n",
    "                h_i = 1/(1 + np.exp(-np.dot(theta, x_i)))  # score\n",
    "                gradient = np.multiply((h_i - y_i), x_i)\n",
    "                theta = theta - learning_rate * gradient\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        you will compute e ^ (log(p(c)) + sum(log(p(w_i | c))) here\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        return a dictionary of the values of P(data | c)  for each class,\n",
    "        as in section 4.3 of the textbook e.g. {\"0\": 0.000061, \"1\": 0.000032}\n",
    "        \"\"\"\n",
    "        features = self.featurize(data)\n",
    "        assert (len(self.weights) == len(features))\n",
    "        feature_vals = [f[1] for f in features]\n",
    "\n",
    "        dot_prod = np.dot(self.weights, feature_vals)\n",
    "        score = 1 / (1 + math.exp(-1 * dot_prod))\n",
    "        return score\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "        prob = self.score(data)\n",
    "        return '0' if prob < 0.5 else '1'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of this class more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "        # features = [pos_words, neg_words, num_nos,\n",
    "        #             num_1_and_2, num_exclam, log_length]\n",
    "\n",
    "        features = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        features[5] = math.log(len(data.split()))\n",
    "        features[4] = data.count('!')\n",
    "\n",
    "        for word in data.split():\n",
    "            if word in self.lexicon:\n",
    "                if float(self.lexicon[word]) > 0:\n",
    "                    features[0] += 1\n",
    "                elif float(self.lexicon[word]) < 0:\n",
    "                    features[1] += 1\n",
    "            if word.lower() == 'no':\n",
    "                features[2] += 1\n",
    "            if word.lower() in [\n",
    "                'me', 'i', 'my', 'myself', 'mine',\n",
    "                'we', 'us', 'our', 'ourselves', 'ours',\n",
    "                'you', 'your', 'yourself', 'yourselves'\n",
    "            ]:\n",
    "                features[3] += 1\n",
    "\n",
    "        return [\n",
    "            ('pos_words', features[0]),\n",
    "            ('neg_words', features[1]),\n",
    "            ('num_nos', features[2]),\n",
    "            ('num_1_2_pronouns', features[3]),\n",
    "            ('num_exclm', features[4]),\n",
    "            ('log_length', features[5]),\n",
    "            ('bias', 1)\n",
    "        ]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression Classifier\"\n",
    "\n",
    "    def describe_experiments(self):\n",
    "        s = \"\"\"\n",
    "    Description of your experiments and their outcomes here.\n",
    "    \"\"\"\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    training = sys.argv[1]\n",
    "    testing = sys.argv[2]\n",
    "\n",
    "    classifier = TextClassify()\n",
    "    print(classifier)\n",
    "    # do the things that you need to with your base class\n",
    "\n",
    "    # report precision, recall, f1\n",
    "\n",
    "    improved = TextClassifyImproved()\n",
    "    print(improved)\n",
    "\n",
    "    # do the things that you need to with your improved class\n",
    "    examples_train = generate_tuples_from_file(training)\n",
    "    improved.train(examples_train)\n",
    "    print(f'LEARNED WEIGHTS: {improved.weights}')\n",
    "\n",
    "    examples_dev = generate_tuples_from_file(testing)\n",
    "    y_labels = [e[2] for e in examples_dev]\n",
    "    y_pred = []\n",
    "    for example in examples_dev:\n",
    "        y_pred.append(improved.classify(example[1]))\n",
    "\n",
    "    # report final precision, recall, f1 (for your best model)\n",
    "    # precision = tp / (tp+fp)\n",
    "    final_precision = precision(y_labels, y_pred)\n",
    "    # recall = tp / (tp+fn)\n",
    "    final_recall = recall(y_labels, y_pred)\n",
    "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    final_f1 = f1(y_labels, y_pred)\n",
    "\n",
    "    print(f'FINAL PRECISION: {final_precision}')\n",
    "    print(f'FINAL RECALL: {final_recall}')\n",
    "    print(f'FINAL F1: {final_f1}')\n",
    "\n",
    "    # report a summary of your experiments/features here\n",
    "    print(improved.describe_experiments())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage:\", \"python textclassify_model.py training-file.txt testing-file.txt\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cats are good that is one fuzzy cat a fuzzy cat is not a fluffy dog dogs are happy I like my fluffy dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'cats are good '\n",
    "s += 'that is one fuzzy cat '\n",
    "s += 'a fuzzy cat is not a fluffy dog '\n",
    "s += 'dogs are happy '\n",
    "s += 'I like my fluffy dog'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'cats are good '\n",
    "s += 'that is one fuzzy cat '\n",
    "s += 'a fuzzy cat is not a fluffy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [-2, 1, -4]\n",
    "\n",
    "w = [0, 1, 0]\n",
    "\n",
    "b = 0.8\n",
    "\n",
    "np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [-2, 1, -4]\n",
    "\n",
    "w = [0, 1, 0]\n",
    "\n",
    "b = 0.8\n",
    "\n",
    "np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all unique strings in a\n",
    "unique = set(a)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - bag-of-words baseline\n",
      "Base precision: 0.553763440860215\n",
      "Base recall: 0.9809523809523809\n",
      "Base f1: 0.7079037800687284\n",
      "Logistic Regression Classifier\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.14328484 -0.18776618 -0.04112838 -0.01828445 -0.00435504 -0.01503128\n",
      "  0.00458965]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.14338049 -0.18691384 -0.05432421 -0.01815834 -0.00442221 -0.01648799\n",
      "  0.01139621]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.14346046 -0.18648903 -0.06036364 -0.01808562 -0.00445556 -0.01796295\n",
      "  0.01812064]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.0001\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 0.14389553 -0.18571806 -0.06540108 -0.01785944 -0.00451305 -0.02802813\n",
      "  0.06324355]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7027027027027027\n",
      "FINAL RECALL: 0.7428571428571429\n",
      "FINAL F1: 0.7222222222222223\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.16448167 -0.20494944 -0.09350451 -0.05372674  0.00181123 -0.06814724\n",
      "  0.13602598]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7692307692307693\n",
      "FINAL RECALL: 0.6666666666666666\n",
      "FINAL F1: 0.7142857142857142\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.16559345 -0.20380024 -0.09356542 -0.05293274  0.00148625 -0.09450464\n",
      "  0.25355748]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7692307692307693\n",
      "FINAL RECALL: 0.6666666666666666\n",
      "FINAL F1: 0.7142857142857142\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.16650518 -0.20286141 -0.09360852 -0.05227531  0.00122029 -0.11614031\n",
      "  0.35003396]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7692307692307693\n",
      "FINAL RECALL: 0.6666666666666666\n",
      "FINAL F1: 0.7142857142857142\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.002\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 1.69627686e-01 -1.99671331e-01 -9.37089968e-02 -4.99791030e-02\n",
      "  3.13676136e-04 -1.90390721e-01  6.81134243e-01]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7752808988764045\n",
      "FINAL RECALL: 0.6571428571428571\n",
      "FINAL F1: 0.711340206185567\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.19651127 -0.32188235 -0.13850388 -0.08232534  0.00957138 -0.11957766\n",
      "  0.37510534]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7534246575342466\n",
      "FINAL RECALL: 0.5238095238095238\n",
      "FINAL F1: 0.6179775280898877\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.19866472 -0.3192586  -0.13921127 -0.07972959  0.00877347 -0.17481904\n",
      "  0.61870309]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7567567567567568\n",
      "FINAL RECALL: 0.5333333333333333\n",
      "FINAL F1: 0.6256983240223464\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.19998637 -0.3175446  -0.13957896 -0.07803649  0.00825616 -0.21017063\n",
      "  0.77465385]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7534246575342466\n",
      "FINAL RECALL: 0.5238095238095238\n",
      "FINAL F1: 0.6179775280898877\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.005\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 0.20211948 -0.31454998 -0.1400375  -0.07509822  0.00736431 -0.27041403\n",
      "  1.04062737]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7567567567567568\n",
      "FINAL RECALL: 0.5333333333333333\n",
      "FINAL F1: 0.6256983240223464\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 50\n",
      "LEARNED WEIGHTS: [ 0.15615283 -0.18339821 -0.0742798  -0.03468326 -0.00465065 -0.03838595\n",
      "  0.05939424]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 100\n",
      "LEARNED WEIGHTS: [ 0.15667213 -0.18284614 -0.074279   -0.0343746  -0.00474178 -0.05134873\n",
      "  0.1172606 ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 150\n",
      "LEARNED WEIGHTS: [ 0.15714125 -0.18235097 -0.07425094 -0.03409567 -0.00482371 -0.06305772\n",
      "  0.1695311 ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "-------------------------------------------------\n",
      "LEARNING RATE: 0.001\n",
      "EPOCHS: 500\n",
      "LEARNED WEIGHTS: [ 0.15937584 -0.18001938 -0.07410345 -0.03276121 -0.00521157 -0.11875934\n",
      "  0.418206  ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7352941176470589\n",
      "FINAL RECALL: 0.7142857142857143\n",
      "FINAL F1: 0.7246376811594202\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.711340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.625698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.625698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.724638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  epochs  precision    recall        f1\n",
       "0          0.0001    50.0   0.702703  0.742857  0.722222\n",
       "1          0.0001   100.0   0.702703  0.742857  0.722222\n",
       "2          0.0001   150.0   0.702703  0.742857  0.722222\n",
       "3          0.0001   500.0   0.702703  0.742857  0.722222\n",
       "4          0.0020    50.0   0.769231  0.666667  0.714286\n",
       "5          0.0020   100.0   0.769231  0.666667  0.714286\n",
       "6          0.0020   150.0   0.769231  0.666667  0.714286\n",
       "7          0.0020   500.0   0.775281  0.657143  0.711340\n",
       "8          0.0050    50.0   0.753425  0.523810  0.617978\n",
       "9          0.0050   100.0   0.756757  0.533333  0.625698\n",
       "10         0.0050   150.0   0.753425  0.523810  0.617978\n",
       "11         0.0050   500.0   0.756757  0.533333  0.625698\n",
       "12         0.0010    50.0   0.745098  0.723810  0.734300\n",
       "13         0.0010   100.0   0.745098  0.723810  0.734300\n",
       "14         0.0010   150.0   0.745098  0.723810  0.734300\n",
       "15         0.0010   500.0   0.735294  0.714286  0.724638"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['f1'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# include epochs in the plot\n",
    "df.sort_values(by=['learning_rate'], inplace=True)\n",
    "plt.plot(df['learning_rate'], df['precision'], label='precision', marker='o', color='red')\n",
    "plt.plot(df['learning_rate'], df['recall'], label='recall', marker='o', color='blue')\n",
    "plt.plot(df['learning_rate'], df['f1'], label='f1', marker='o', color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Rate vs Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - bag-of-words baseline\n",
      "Base precision: 0.9130434782608695\n",
      "Base recall: 0.2\n",
      "Base f1: 0.328125\n",
      "Logistic Regression Classifier\n",
      "LEARNED WEIGHTS: [ 0.15667213 -0.18284614 -0.074279   -0.0343746  -0.00474178 -0.05134873\n",
      "  0.1172606 ]\n",
      "\n",
      "    Description of your experiments and their outcomes here.\n",
      "    \n",
      "FINAL PRECISION: 0.7450980392156863\n",
      "FINAL RECALL: 0.7238095238095238\n",
      "FINAL F1: 0.7342995169082125\n",
      "[0.7040816326530612, 0.7450980392156863]\n",
      "[0.7040816326530612, 0.7450980392156863]\n",
      "MULTI CLASS PRECISION: 0.7245898359343738\n",
      "MULTI CLASS RECALL: 0.7250626566416041\n",
      "MULTI CLASS F1: 0.7248261691800537\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: rename this file to textclassify_model.py\n",
    "\n",
    "# feel free to include more imports as needed here\n",
    "# these are the ones that we used for the base model\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Your name and file comment here:\n",
    "Vedanshi Shah & Byron Pham\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Cite your sources here:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Implement your functions that are not methods of the TextClassify class here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_tuples_from_file(training_file_path):\n",
    "    \"\"\"\n",
    "    Generates tuples from file formated like:\n",
    "    id\\ttext\\tlabel\n",
    "    Parameters:\n",
    "      training_file_path - str path to file to read in\n",
    "    Return:\n",
    "      a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "    \"\"\"\n",
    "    f = open(training_file_path, \"r\", encoding=\"utf8\")\n",
    "    listOfExamples = []\n",
    "    for review in f:\n",
    "        if len(review.strip()) == 0:\n",
    "            continue\n",
    "        dataInReview = review.split(\"\\t\")\n",
    "        for i in range(len(dataInReview)):\n",
    "            # remove any extraneous whitespace\n",
    "            dataInReview[i] = dataInReview[i].strip()\n",
    "        t = tuple(dataInReview)\n",
    "        listOfExamples.append(t)\n",
    "    f.close()\n",
    "    return listOfExamples\n",
    "\n",
    "\n",
    "def precision(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the precision for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double precision (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and predicted_labels[i] == '1':\n",
    "            true_pos += 1\n",
    "        elif gold_labels[i] == '0' and predicted_labels[i] == '1':\n",
    "            false_pos += 1\n",
    "\n",
    "    if true_pos == 0 and false_pos == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the recall for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double recall (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1' and predicted_labels[i] == '1':\n",
    "            true_pos += 1\n",
    "        elif gold_labels[i] == '1' and predicted_labels[i] == '0':\n",
    "            false_neg += 1\n",
    "    if true_pos == 0 and false_neg == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the f1 for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double f1 (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    # F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    precision_val = precision(gold_labels, predicted_labels)\n",
    "    recall_val = recall(gold_labels, predicted_labels)\n",
    "    if precision_val + recall_val == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = (2 * precision_val * recall_val) / (precision_val + recall_val)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def precision_multiclass(gold_labels, classified_labels):\n",
    "    \"\"\" Gold labels is a list of strings of the true labels\n",
    "        Classified labels is a list of strings of the labels assigned by the classifier\n",
    "        Returns the precision as a float\n",
    "\n",
    "    Args:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        classified_labels (list): a corresponding list of labels predicted by the system\n",
    "\n",
    "    Returns:\n",
    "        float: precision from 0 to 1\n",
    "    \"\"\"\n",
    "    # classes = set(gold_labels) | set(classified_labels)\n",
    "    # tp = {c: 0 for c in classes}\n",
    "    # fp = {c: 0 for c in classes}\n",
    "    # for gold, classified in zip(gold_labels, classified_labels):\n",
    "    #     if gold == classified:\n",
    "    #         tp[gold] += 1\n",
    "    #     else:\n",
    "    #         fp[classified] += 1\n",
    "    # precision_scores = [tp[c] / (tp[c] + fp[c])\n",
    "    #                     if tp[c] + fp[c] > 0 else 0 for c in classes]\n",
    "    # macro_precision = sum(precision_scores) / len(precision_scores)\n",
    "    # return macro_precision\n",
    "    unique_labels = set(gold_labels + classified_labels)\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "\n",
    "        precisions.append(len([i for i in range(len(gold_labels)) if (\n",
    "            gold_labels[i] == classified_labels[i] and gold_labels[i] == label)])/classified_labels.count(label))\n",
    "\n",
    "    if len(unique_labels) == 0:\n",
    "\n",
    "        multi_precision = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        multi_precision = sum(precisions) / len(unique_labels)\n",
    "\n",
    "    print(precisions)\n",
    "\n",
    "    return multi_precision\n",
    "\n",
    "\n",
    "def recall_multi(gold_labels, classified_labels):\n",
    "    \"\"\"gold labels is a list of strings of the true labels\n",
    "        classified labels is a list of strings of the labels assigned by the classifier\n",
    "        returns the recall as a float\n",
    "\n",
    "    Args:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        classified_labels (list): a corresponding list of labels predicted by the system\n",
    "\n",
    "    Returns:\n",
    "        float: recall as a float\n",
    "    \"\"\"\n",
    "    unique_labels = set(gold_labels + classified_labels)\n",
    "    recalls = []\n",
    "    for label in unique_labels:\n",
    "        recalls.append(len([i for i in range(len(gold_labels)) if (\n",
    "            gold_labels[i] == classified_labels[i] and gold_labels[i] == label)])/gold_labels.count(label))\n",
    "    if len(unique_labels) == 0:\n",
    "        multi_recall = 0\n",
    "    else:\n",
    "        multi_recall = sum(recalls) / len(unique_labels)\n",
    "    return multi_recall\n",
    "\n",
    "\n",
    "def f1_multi(gold_labels, classified_labels):\n",
    "    \"\"\"gold labels is a list of strings of the true labels\n",
    "        classified labels is a list of strings of the labels assigned by the classifier\n",
    "        returns the f1 as a float\n",
    "\n",
    "    Args:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        classified_labels (list): a corresponding list of labels predicted by the system\n",
    "\n",
    "    Returns:\n",
    "        float: f1 as a float\n",
    "    \"\"\"\n",
    "    precision = precision_multiclass(gold_labels, classified_labels)\n",
    "    recall = recall_multi(gold_labels, classified_labels)\n",
    "    multi_f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    return multi_f1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement any other non-required functions here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "implement your TextClassify class here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextClassify:\n",
    "\n",
    "    def __init__(self):\n",
    "        # do whatever you need to do to set up your class here\n",
    "        self.words_0 = Counter()\n",
    "        self.words_1 = Counter()\n",
    "\n",
    "        self.prior_0 = 0\n",
    "        self.prior_1 = 0\n",
    "\n",
    "        self.word_data = {0: self.words_0, 1: self.words_1}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        # TODO: Ask if this should be a self. or not\n",
    "\n",
    "        # calculate prior for each class\n",
    "        count_0 = 0\n",
    "        count_1 = 0\n",
    "        for example in examples:\n",
    "            if example[2] == \"0\":\n",
    "                count_0 += 1\n",
    "            else:\n",
    "                count_1 += 1\n",
    "\n",
    "        self.prior_0 = count_0 / len(examples)\n",
    "        self.prior_1 = count_1 / len(examples)\n",
    "\n",
    "        # update bag of words counts in self.word_data\n",
    "        # word_data format:\n",
    "        # { 0: Counter(), 1: Counter() }\n",
    "\n",
    "        for example in examples:\n",
    "            words = example[1].split()\n",
    "            self.vocab.update(words)\n",
    "            if example[2] == '0':\n",
    "                self.words_0.update(Counter(words))\n",
    "            else:\n",
    "                self.words_1.update(Counter(words))\n",
    "\n",
    "        self.word_data = {'0': self.words_0, '1': self.words_1}\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        \"\"\"\n",
    "        word_probs = {'0': 1, '1': 1}\n",
    "\n",
    "        word_list = data.split()\n",
    "\n",
    "        for word in word_list:\n",
    "            if word in self.words_0:\n",
    "                word_probs['0'] *= (self.words_0[word] + 1) / \\\n",
    "                    (sum(self.words_0.values()) + len(self.vocab))\n",
    "            elif word in self.vocab:\n",
    "                word_probs['0'] *= 1 / \\\n",
    "                    (sum(self.words_0.values()) + len(self.vocab))\n",
    "\n",
    "            if word in self.words_1:\n",
    "                word_probs['1'] *= (self.words_1[word] + 1) / \\\n",
    "                    (sum(self.words_1.values()) + len(self.vocab))\n",
    "            elif word in self.vocab:\n",
    "                word_probs['1'] *= 1 / \\\n",
    "                    (sum(self.words_1.values()) + len(self.vocab))\n",
    "\n",
    "        # multiply these by the prior\n",
    "        word_probs['0'] *= self.prior_0\n",
    "        word_probs['1'] *= self.prior_1\n",
    "\n",
    "        return word_probs\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "\n",
    "        score = self.score(data)\n",
    "        # greater = 0\n",
    "        # else = 1\n",
    "        # else = 0 (if equal)\n",
    "        if score['0'] > score['1']:\n",
    "            return '0'\n",
    "        elif score['1'] > score['0']:\n",
    "            return '1'\n",
    "        else:\n",
    "            return '0'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of your TextClassifyImproved model more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "\n",
    "        data_list = data.split()\n",
    "        return [(d, True) for d in data_list]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Naive Bayes - bag-of-words baseline\"\n",
    "\n",
    "\n",
    "def k_fold(all_examples, k):\n",
    "    \"\"\"\"\n",
    "    all examples is a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "    containing all examples from the train and dev sets\n",
    "\n",
    "    @return a list of lists containing k sublists where each sublist is one \"fold\" in the given data\n",
    "    \"\"\"\n",
    "    # Shuffle the examples randomly\n",
    "    random.shuffle(all_examples)\n",
    "\n",
    "    # Calculate the size of each fold\n",
    "    fold_size = len(all_examples) // k\n",
    "\n",
    "    # Initialize a list to hold the folds\n",
    "    folds = []\n",
    "\n",
    "    # Split the examples into k folds\n",
    "    for i in range(k):\n",
    "        # Calculate the start and end indices for the current fold\n",
    "        start_index = i * fold_size\n",
    "        end_index = (i + 1) * fold_size\n",
    "\n",
    "        # Create the current fold by taking a slice of the shuffled examples\n",
    "        current_fold = all_examples[start_index:min(\n",
    "            len(all_examples), end_index)]\n",
    "\n",
    "        # Add the current fold to the list of folds\n",
    "        folds.append(current_fold)\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "class TextClassifyImproved:\n",
    "    # count(positive words), count(negative words), 'no' in str,\n",
    "    # count of 1st and 2nd pronouns, if ! doc, log of length\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lexicon = self.read_lexicon('vader_lexicon.txt')\n",
    "        self.weights = []\n",
    "\n",
    "    def read_lexicon(self, filepath) -> dict:\n",
    "        output = {}\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split('\\t')\n",
    "                output[l[0]] = l[1]\n",
    "\n",
    "        return output\n",
    "\n",
    "        # # print(f' the examplesssss aree: {all_examples}')\n",
    "        # n = len(all_examples)\n",
    "        # fold_size = n // k  # integer division to get the size of each fold\n",
    "        # remainder = n % k  # get the remainder for the last fold\n",
    "\n",
    "        # # initialize the list of k folds\n",
    "        # folds = [[] for _ in range(k)]\n",
    "\n",
    "        # # iterate over the examples and add them to the appropriate fold\n",
    "        # start_idx = 0\n",
    "        # for i in range(k):\n",
    "        #     # get the size of the current fold\n",
    "        #     current_fold_size = fold_size + (1 if i < remainder else 0)\n",
    "\n",
    "        #     # add the examples to the current fold\n",
    "        #     end_idx = start_idx + current_fold_size\n",
    "        #     folds[i] = all_examples[start_idx:end_idx]\n",
    "\n",
    "        #     # update the starting index for the next fold\n",
    "        #     start_idx = end_idx\n",
    "\n",
    "        # return folds\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # ft = k_fold(examples, 10)\n",
    "        # theta_lst = []\n",
    "        # print(ft)\n",
    "\n",
    "        # for feat in ft:\n",
    "        #     # print(f' the feat are: {feat}')\n",
    "        #     docs = [feat[1] for feat in feat]\n",
    "        #     labels = [feat[2] for feat in feat]\n",
    "\n",
    "        #     print(docs)\n",
    "        #     # featurize the training data\n",
    "        #     docs_featurized = [f[1] for f in self.featurize(docs)]\n",
    "\n",
    "        #     # create a vocabulary from the training data\n",
    "        #     # set of all unique words in the training data\n",
    "        #     vocab = set()\n",
    "        #     for doc in docs:\n",
    "        #         vocab.update(doc.split())\n",
    "\n",
    "        #     y = [1 if label == '1' else 0 for label in labels]\n",
    "        #     theta_train = self.train_logistic_regression(docs_featurized, y)\n",
    "        #     theta_lst.append(theta_train)\n",
    "        #     # average\n",
    "        #     theta = np.mean(theta_lst)\n",
    "\n",
    "        #     # print f1, precision, recall\n",
    "        #     print(f'F1: {f1(y, theta)}')\n",
    "        #     print(f'Precision: {precision(y, theta)}')\n",
    "        #     print(f'Recall: {recall(y, theta)}')\n",
    "\n",
    "        # self.theta = theta\n",
    "\n",
    "        # # print(f'feat is {feat}')\n",
    "        # # print('  ')\n",
    "        docs = [feat[1] for feat in examples]\n",
    "        labels = [feat[2] for feat in examples]\n",
    "\n",
    "        # featurize the training data\n",
    "        docs_featurized = [[f[1]\n",
    "                            for f in self.featurize(doc)] for doc in docs]\n",
    "\n",
    "        # create a vocabulary from the training data\n",
    "        # set of all unique words in the training data\n",
    "        vocab = set()\n",
    "        for doc in docs:\n",
    "            vocab.update(doc.split())\n",
    "\n",
    "        y = [1 if label == '1' else 0 for label in labels]\n",
    "\n",
    "        # train the model with logistic regression\n",
    "        # print(docs_featurized)\n",
    "        theta_train = self.train_logistic_regression(docs_featurized, y)\n",
    "        # theta_lst.append(theta_train)\n",
    "        # average\n",
    "        # theta = np.mean(theta_lst)\n",
    "\n",
    "        # print f1, precision, recall\n",
    "        # print(f'F1: {f1(y, theta)}')\n",
    "        # print(f'Precision: {precision(y, theta)}')\n",
    "        # print(f'Recall: {recall(y, theta)}')\n",
    "\n",
    "        self.weights = theta_train\n",
    "\n",
    "    def train_logistic_regression(self, x, y, learning_rate=0.001, num_epoch=100):\n",
    "        # initialise\n",
    "        n_features = len(x[0])\n",
    "        theta = np.zeros(n_features)\n",
    "\n",
    "        # perform gradient descent\n",
    "        for epoch in range(num_epoch):\n",
    "            for i in range(len(x)):\n",
    "                x_i = x[i]  # features\n",
    "                y_i = y[i]  # label\n",
    "                h_i = 1/(1 + np.exp(-np.dot(theta, x_i)))  # score\n",
    "                gradient = np.multiply((h_i - y_i), x_i)\n",
    "                theta = theta - learning_rate * gradient\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        you will compute e ^ (log(p(c)) + sum(log(p(w_i | c))) here\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        return a dictionary of the values of P(data | c)  for each class,\n",
    "        as in section 4.3 of the textbook e.g. {\"0\": 0.000061, \"1\": 0.000032}\n",
    "        \"\"\"\n",
    "        features = self.featurize(data)\n",
    "        assert (len(self.weights) == len(features))\n",
    "        feature_vals = [f[1] for f in features]\n",
    "\n",
    "        dot_prod = np.dot(self.weights, feature_vals)\n",
    "        score = 1 / (1 + math.exp(-1 * dot_prod))\n",
    "        return score\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "        prob = self.score(data)\n",
    "        return '0' if prob <= 0.5 else '1'\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of this class more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "        # features = [pos_words, neg_words, num_nos,\n",
    "        #             num_1_and_2, num_exclam, log_length]\n",
    "\n",
    "        features = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        features[5] = math.log(len(data.split()))\n",
    "        features[4] = data.count('!')\n",
    "\n",
    "        for word in data.split():\n",
    "            if word in self.lexicon:\n",
    "                if float(self.lexicon[word]) > 0:\n",
    "                    features[0] += 1\n",
    "                elif float(self.lexicon[word]) < 0:\n",
    "                    features[1] += 1\n",
    "            if word.lower() == 'no':\n",
    "                features[2] += 1\n",
    "            if word.lower() in [\n",
    "                'me', 'i', 'my', 'myself', 'mine',\n",
    "                'we', 'us', 'our', 'ourselves', 'ours',\n",
    "                'you', 'your', 'yourself', 'yourselves'\n",
    "            ]:\n",
    "                features[3] += 1\n",
    "\n",
    "        return [\n",
    "            ('pos_words', features[0]),\n",
    "            ('neg_words', features[1]),\n",
    "            ('num_nos', features[2]),\n",
    "            ('num_1_2_pronouns', features[3]),\n",
    "            ('num_exclm', features[4]),\n",
    "            ('log_length', features[5]),\n",
    "            ('bias', 1)\n",
    "        ]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression Classifier\"\n",
    "\n",
    "    def describe_experiments(self):\n",
    "        s = \"\"\"\n",
    "    Description of your experiments and their outcomes here.\n",
    "    \"\"\"\n",
    "        return s\n",
    "\n",
    "\n",
    "training = 'training_files/movie_reviews_train.txt'\n",
    "testing = 'training_files/movie_reviews_dev.txt'\n",
    "\n",
    "classifier = TextClassify()\n",
    "print(classifier)\n",
    "# do the things that you need to with your base class\n",
    "examples_train_base = generate_tuples_from_file(training)\n",
    "classifier.train(examples_train_base)\n",
    "examples_dev_base = generate_tuples_from_file(testing)\n",
    "y_labels = [e[2] for e in examples_dev_base]\n",
    "y_pred = []\n",
    "for example in examples_dev_base:\n",
    "    y_pred.append(classifier.classify(example[1]))\n",
    "\n",
    "# report precision, recall, f1\n",
    "base_precision = precision(y_labels, y_pred)\n",
    "base_recall = recall(y_labels, y_pred)\n",
    "base_f1 = f1(y_labels, y_pred)\n",
    "\n",
    "print(f'Base precision: {base_precision}')\n",
    "print(f'Base recall: {base_recall}')\n",
    "print(f'Base f1: {base_f1}')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Improved model\n",
    "improved = TextClassifyImproved()\n",
    "print(improved)\n",
    "\n",
    "# do the things that you need to with your improved class\n",
    "examples_train = generate_tuples_from_file(training)\n",
    "improved.train(examples_train)\n",
    "print(f'LEARNED WEIGHTS: {improved.weights}')\n",
    "\n",
    "examples_dev = generate_tuples_from_file(testing)\n",
    "y_labels = [e[2] for e in examples_dev]\n",
    "y_pred = []\n",
    "for example in examples_dev:\n",
    "    y_pred.append(improved.classify(example[1]))\n",
    "\n",
    "# report a summary of your experiments/features here\n",
    "print(improved.describe_experiments())\n",
    "\n",
    "# report final precision, recall, f1 (for your best model)\n",
    "# precision = tp / (tp+fp)\n",
    "final_precision = precision(y_labels, y_pred)\n",
    "# recall = tp / (tp+fn)\n",
    "final_recall = recall(y_labels, y_pred)\n",
    "# f1 = 2 * (precision * recall) / (precision + recall)\n",
    "final_f1 = f1(y_labels, y_pred)\n",
    "\n",
    "print(f'FINAL PRECISION: {final_precision}')\n",
    "print(f'FINAL RECALL: {final_recall}')\n",
    "print(f'FINAL F1: {final_f1}')\n",
    "\n",
    "# report multi class precision, recall, f1 (for your best model)\n",
    "multi_precision = precision_multiclass(y_labels, y_pred)\n",
    "# recall = tp / (tp+fn)\n",
    "multi_recall = recall_multi(y_labels, y_pred)\n",
    "# f1 = 2 * (precision * recall) / (precision + recall)\n",
    "multi_f1 = f1_multi(y_labels, y_pred)\n",
    "\n",
    "print(f'MULTI CLASS PRECISION: {multi_precision}')\n",
    "print(f'MULTI CLASS RECALL: {multi_recall}')\n",
    "print(f'MULTI CLASS F1: {multi_f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_lst.index(max(f1_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7222222222222223,\n",
       " 0.7222222222222223,\n",
       " 0.7222222222222223,\n",
       " 0.7222222222222223,\n",
       " 0.7142857142857142,\n",
       " 0.7142857142857142,\n",
       " 0.7142857142857142,\n",
       " 0.711340206185567,\n",
       " 0.6179775280898877,\n",
       " 0.6256983240223464,\n",
       " 0.6179775280898877,\n",
       " 0.6256983240223464,\n",
       " 0.7342995169082125,\n",
       " 0.7342995169082125,\n",
       " 0.7342995169082125,\n",
       " 0.7246376811594202]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the index of the best model\n",
    "f1_lst.index(max(f1_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6973733858775077"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean the list\n",
    "np.mean(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7342995169082125,\n",
       " 0.7342995169082125,\n",
       " 0.7342995169082125,\n",
       " 0.7246376811594202,\n",
       " 0.5308641975308641,\n",
       " 0.5341614906832298,\n",
       " 0.5341614906832298,\n",
       " 0.525,\n",
       " 0.08928571428571427,\n",
       " 0.3206106870229008,\n",
       " 0.08928571428571427,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [learning_rate, epochs, precision, recall, f1]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caa917194a3f9b49aa7c5f08ecf9e4b4c66d3db34522bc007e94fdaaccb6f325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
